{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "root = \"\"\n",
    "chapter_id =\"Tuning\"\n",
    "# path of folder\n",
    "images_path = os.path.join(root,\"images\",chapter_id)\n",
    "os.makedirs(images_path,exist_ok=True)\n",
    "def save_fig(fig_id,tigh_layout,fig_extension=\"png\",resolution=True):\n",
    "    # path until file name\n",
    "    path = os.path.join(images_path,fig_id + \".\" +fig_extension)\n",
    "    print(\"save figure\",fig_id)\n",
    "    if tigh_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path,fig_extension=fig_extension,resolution=resolution)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _california_housing_dataset:\n",
      "\n",
      "California Housing dataset\n",
      "--------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 20640\n",
      "\n",
      "    :Number of Attributes: 8 numeric, predictive attributes and the target\n",
      "\n",
      "    :Attribute Information:\n",
      "        - MedInc        median income in block group\n",
      "        - HouseAge      median house age in block group\n",
      "        - AveRooms      average number of rooms per household\n",
      "        - AveBedrms     average number of bedrooms per household\n",
      "        - Population    block group population\n",
      "        - AveOccup      average number of household members\n",
      "        - Latitude      block group latitude\n",
      "        - Longitude     block group longitude\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "This dataset was obtained from the StatLib repository.\n",
      "https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n",
      "\n",
      "The target variable is the median house value for California districts,\n",
      "expressed in hundreds of thousands of dollars ($100,000).\n",
      "\n",
      "This dataset was derived from the 1990 U.S. census, using one row per census\n",
      "block group. A block group is the smallest geographical unit for which the U.S.\n",
      "Census Bureau publishes sample data (a block group typically has a population\n",
      "of 600 to 3,000 people).\n",
      "\n",
      "An household is a group of people residing within a home. Since the average\n",
      "number of rooms and bedrooms in this dataset are provided per household, these\n",
      "columns may take surpinsingly large values for block groups with few households\n",
      "and many empty houses, such as vacation resorts.\n",
      "\n",
      "It can be downloaded/loaded using the\n",
      ":func:`sklearn.datasets.fetch_california_housing` function.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
      "      Statistics and Probability Letters, 33 (1997) 291-297\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load data set\n",
    "import tensorflow as tf \n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "print(housing.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT DATA TRAIN FULL AND TESTING \n",
    "X_train_full,X_test,y_train_full,y_test = train_test_split(housing['data'],housing['target'],random_state=42)\n",
    "#SPLIT DATA TRAIN FULL TO TRAIN AND VALIDATION\n",
    "X_train,X_valid,y_train,y_valid = train_test_split(X_train_full,y_train_full,random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.fit_transform(X_valid)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regresion model\n",
    "def build_model(n_hidden=1, n_neurons=30,learning_rate=3e-3,input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons,activation='relu'))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    model.compile(loss='mse',optimizer=optimizer)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wrap class \n",
    "-> agar model dapat menggunakan libray dari sklearn \n",
    "\n",
    "class KerasClassifier: Implementation of the scikit-learn classifier API for Keras.\n",
    "\n",
    "class KerasRegressor: Implementation of the scikit-learn regressor API for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11528\\1709004121.py:1: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n"
     ]
    }
   ],
   "source": [
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 2s 1ms/step - loss: 1.0896 - val_loss: 0.8275\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 871us/step - loss: 0.7606 - val_loss: 0.6074\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 858us/step - loss: 0.5456 - val_loss: 0.5189\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 858us/step - loss: 0.4732 - val_loss: 0.4983\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 859us/step - loss: 0.4503 - val_loss: 0.4779\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 856us/step - loss: 0.4338 - val_loss: 0.4677\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 855us/step - loss: 0.4241 - val_loss: 0.4609\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 861us/step - loss: 0.4168 - val_loss: 0.4580\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 862us/step - loss: 0.4108 - val_loss: 0.4497\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 872us/step - loss: 0.4060 - val_loss: 0.4462\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 841us/step - loss: 0.4021 - val_loss: 0.4443\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 871us/step - loss: 0.3984 - val_loss: 0.4421\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 867us/step - loss: 0.3951 - val_loss: 0.4418\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 868us/step - loss: 0.3921 - val_loss: 0.4414\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 863us/step - loss: 0.3894 - val_loss: 0.4371\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 875us/step - loss: 0.3869 - val_loss: 0.4349\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 900us/step - loss: 0.3848 - val_loss: 0.4339\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 872us/step - loss: 0.3829 - val_loss: 0.4337\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 864us/step - loss: 0.3807 - val_loss: 0.4334\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 867us/step - loss: 0.3791 - val_loss: 0.4316\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 855us/step - loss: 0.3774 - val_loss: 0.4336\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 859us/step - loss: 0.3756 - val_loss: 0.4346\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 865us/step - loss: 0.3742 - val_loss: 0.4293\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 859us/step - loss: 0.3725 - val_loss: 0.4320\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 845us/step - loss: 0.3710 - val_loss: 0.4280\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 879us/step - loss: 0.3700 - val_loss: 0.4272\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 862us/step - loss: 0.3691 - val_loss: 0.4268\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 866us/step - loss: 0.3677 - val_loss: 0.4275\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 863us/step - loss: 0.3670 - val_loss: 0.4254\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 859us/step - loss: 0.3653 - val_loss: 0.4268\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 862us/step - loss: 0.3647 - val_loss: 0.4235\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 872us/step - loss: 0.3633 - val_loss: 0.4280\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 883us/step - loss: 0.3632 - val_loss: 0.4238\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 913us/step - loss: 0.3617 - val_loss: 0.4238\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 860us/step - loss: 0.3610 - val_loss: 0.4249\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 861us/step - loss: 0.3608 - val_loss: 0.4239\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 935us/step - loss: 0.3596 - val_loss: 0.4219\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 858us/step - loss: 0.3589 - val_loss: 0.4230\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 865us/step - loss: 0.3582 - val_loss: 0.4209\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 866us/step - loss: 0.3572 - val_loss: 0.4188\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 899us/step - loss: 0.3570 - val_loss: 0.4182\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 863us/step - loss: 0.3563 - val_loss: 0.4188\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 856us/step - loss: 0.3552 - val_loss: 0.4203\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 868us/step - loss: 0.3548 - val_loss: 0.4184\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 870us/step - loss: 0.3543 - val_loss: 0.4178\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 883us/step - loss: 0.3532 - val_loss: 0.4170\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 0s 868us/step - loss: 0.3527 - val_loss: 0.4151\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 0s 862us/step - loss: 0.3521 - val_loss: 0.4143\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 864us/step - loss: 0.3525 - val_loss: 0.4143\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 0s 850us/step - loss: 0.3510 - val_loss: 0.4179\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 0s 859us/step - loss: 0.3504 - val_loss: 0.4149\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 0s 857us/step - loss: 0.3502 - val_loss: 0.4173\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 0s 855us/step - loss: 0.3496 - val_loss: 0.4172\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 0s 858us/step - loss: 0.3497 - val_loss: 0.4141\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 0s 854us/step - loss: 0.3490 - val_loss: 0.4134\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 0s 856us/step - loss: 0.3485 - val_loss: 0.4143\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 0s 864us/step - loss: 0.3479 - val_loss: 0.4136\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 0s 861us/step - loss: 0.3470 - val_loss: 0.4170\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 0s 864us/step - loss: 0.3475 - val_loss: 0.4140\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 0s 861us/step - loss: 0.3465 - val_loss: 0.4145\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 0s 864us/step - loss: 0.3452 - val_loss: 0.4148\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 0s 859us/step - loss: 0.3453 - val_loss: 0.4091\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 0s 860us/step - loss: 0.3444 - val_loss: 0.4150\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 0s 925us/step - loss: 0.3450 - val_loss: 0.4094\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 0s 857us/step - loss: 0.3437 - val_loss: 0.4091\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 0s 868us/step - loss: 0.3431 - val_loss: 0.4130\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 0s 881us/step - loss: 0.3428 - val_loss: 0.4081\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 0s 869us/step - loss: 0.3423 - val_loss: 0.4087\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 0s 866us/step - loss: 0.3419 - val_loss: 0.4104\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 0s 860us/step - loss: 0.3413 - val_loss: 0.4080\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 0s 872us/step - loss: 0.3414 - val_loss: 0.4095\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 0s 865us/step - loss: 0.3405 - val_loss: 0.4099\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 0s 873us/step - loss: 0.3399 - val_loss: 0.4063\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 0s 865us/step - loss: 0.3402 - val_loss: 0.4078\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 0s 869us/step - loss: 0.3397 - val_loss: 0.4145\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 0s 858us/step - loss: 0.3394 - val_loss: 0.4065\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 0s 947us/step - loss: 0.3383 - val_loss: 0.4079\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 0s 887us/step - loss: 0.3384 - val_loss: 0.4095\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 0s 865us/step - loss: 0.3383 - val_loss: 0.4086\n",
      "Epoch 80/100\n",
      "363/363 [==============================] - 0s 887us/step - loss: 0.3376 - val_loss: 0.4059\n",
      "Epoch 81/100\n",
      "363/363 [==============================] - 0s 891us/step - loss: 0.3384 - val_loss: 0.4035\n",
      "Epoch 82/100\n",
      "363/363 [==============================] - 0s 874us/step - loss: 0.3371 - val_loss: 0.4062\n",
      "Epoch 83/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3368 - val_loss: 0.4061\n",
      "Epoch 84/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3362 - val_loss: 0.4054\n",
      "Epoch 85/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3369 - val_loss: 0.4079\n",
      "Epoch 86/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3359 - val_loss: 0.4061\n",
      "Epoch 87/100\n",
      "363/363 [==============================] - 0s 928us/step - loss: 0.3356 - val_loss: 0.4075\n",
      "Epoch 88/100\n",
      "363/363 [==============================] - 0s 899us/step - loss: 0.3358 - val_loss: 0.4036\n",
      "Epoch 89/100\n",
      "363/363 [==============================] - 0s 895us/step - loss: 0.3346 - val_loss: 0.4053\n",
      "Epoch 90/100\n",
      "363/363 [==============================] - 0s 902us/step - loss: 0.3353 - val_loss: 0.4063\n",
      "Epoch 91/100\n",
      "363/363 [==============================] - 0s 900us/step - loss: 0.3340 - val_loss: 0.4094\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b98e7ceee0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_reg.fit(X_train,y_train,epochs=100,\n",
    "              validation_data=[X_valid,y_valid],\n",
    "              callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 696us/step - loss: 0.3718\n"
     ]
    }
   ],
   "source": [
    "mse_test = keras_reg.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.64688206, 1.642302  , 4.772844  ], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X_test[:3]\n",
    "y_pred = keras_reg.predict(X_new)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.477  , 0.458  , 5.00001])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 814us/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = keras_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6612818771164024"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[96.61036220656865,\n",
       " 97.88060632725822,\n",
       " 92.40666400403123,\n",
       " 96.12474044941891,\n",
       " 95.58817519130913,\n",
       " 98.25411258886575,\n",
       " 99.01540759060205,\n",
       " 90.1153497908005,\n",
       " 96.6237680060932,\n",
       " 90.49287486699858,\n",
       " 95.35794968563704,\n",
       " 92.76921065285431,\n",
       " 92.95651573338,\n",
       " 93.40992101020801,\n",
       " 96.08847322344373,\n",
       " 93.22401376896615,\n",
       " 97.22299136009003,\n",
       " 93.9188286551209,\n",
       " 90.65049509184983,\n",
       " 97.74744021720154,\n",
       " 92.75089163557213,\n",
       " 94.19868406742253,\n",
       " 96.74013775071772,\n",
       " 93.20836048854954,\n",
       " 90.53817312339861,\n",
       " 93.61645643649642,\n",
       " 99.41641690301806,\n",
       " 96.29564912378136,\n",
       " 96.59790558679582,\n",
       " 96.19979667257857,\n",
       " 91.90685189127967,\n",
       " 94.05554781781616,\n",
       " 97.41001947527522,\n",
       " 90.96641487528532,\n",
       " 92.67366855332948,\n",
       " 92.65869480931258,\n",
       " 94.19126106945197,\n",
       " 99.79337780043105,\n",
       " 90.64237087767941,\n",
       " 95.055437551229,\n",
       " 91.71698867346237,\n",
       " 99.69152334033319,\n",
       " 91.08083291389114,\n",
       " 93.90966222911773,\n",
       " 97.27612780880129,\n",
       " 96.93511994599605,\n",
       " 94.09910896241736,\n",
       " 93.3466175581908,\n",
       " 93.85042725628963,\n",
       " 92.54120027138087,\n",
       " 91.9682726098703,\n",
       " 94.69895657806406,\n",
       " 92.58272094192371,\n",
       " 92.76753865071113,\n",
       " 96.44817600187737,\n",
       " 99.66905653349365,\n",
       " 95.90989884488772,\n",
       " 90.73292187003553,\n",
       " 90.71957895873119,\n",
       " 99.48950005267578,\n",
       " 92.86365863587363,\n",
       " 90.87726823243798,\n",
       " 95.8635011781738,\n",
       " 96.1123314997496,\n",
       " 96.36436470942728,\n",
       " 92.57168208569814,\n",
       " 90.14340037466063,\n",
       " 99.63207862398815,\n",
       " 92.4108033327974,\n",
       " 96.64419566364172,\n",
       " 96.95631902990814,\n",
       " 95.97429223132491,\n",
       " 93.0165798050863,\n",
       " 92.60772036229939,\n",
       " 95.84960119739877,\n",
       " 98.59908216526824,\n",
       " 99.44035569730089,\n",
       " 91.00986984706863,\n",
       " 91.48019427057083,\n",
       " 99.41943023056074,\n",
       " 97.26229505302601,\n",
       " 98.77478454623423,\n",
       " 91.94211085389315,\n",
       " 95.74783537002119,\n",
       " 96.90025006525812,\n",
       " 96.68580614195062,\n",
       " 93.95469922285464,\n",
       " 90.14609969125922,\n",
       " 95.7008385085686,\n",
       " 92.43230872090992,\n",
       " 94.37240305390887,\n",
       " 99.55406862727503,\n",
       " 93.86449851691697,\n",
       " 98.32630599483512,\n",
       " 91.80570170504576,\n",
       " 96.60789388943189,\n",
       " 99.75803728913806,\n",
       " 90.97140078014833,\n",
       " 90.07892799512281,\n",
       " 94.20678220706036,\n",
       " 90.8826159289107,\n",
       " 97.38379600168142,\n",
       " 99.10372105798622,\n",
       " 94.21113154055097,\n",
       " 92.48696006905654,\n",
       " 94.21492683735309,\n",
       " 97.12827285515779,\n",
       " 90.08589656601539,\n",
       " 95.76668288013724,\n",
       " 96.0075081685507,\n",
       " 96.25460660484472,\n",
       " 92.32447197073843,\n",
       " 97.03217586732926,\n",
       " 90.87080063634876,\n",
       " 91.90954581551348,\n",
       " 98.71731552014343,\n",
       " 97.28471860314083,\n",
       " 90.1304359076414,\n",
       " 92.38630202727559,\n",
       " 92.0562357021088,\n",
       " 92.60489154302093,\n",
       " 92.37835476201296,\n",
       " 90.5948595057303,\n",
       " 94.45881126117686,\n",
       " 97.22334398375811,\n",
       " 95.9411744699901,\n",
       " 96.61207914453034,\n",
       " 90.77279168222286,\n",
       " 99.49020749624742,\n",
       " 98.312738064041,\n",
       " 97.96736386669515,\n",
       " 98.15223271068065,\n",
       " 99.29354167601407,\n",
       " 95.31169766724034,\n",
       " 91.91934309324425,\n",
       " 96.04283686660109,\n",
       " 97.32735082415446,\n",
       " 97.2764599663252,\n",
       " 95.08296330125751,\n",
       " 90.65150290784304,\n",
       " 93.5887628158922,\n",
       " 99.16866773937844,\n",
       " 95.71619480340117,\n",
       " 95.2522467320581,\n",
       " 92.58584009716,\n",
       " 93.56249778690989,\n",
       " 98.90341778067214,\n",
       " 96.54686717808833,\n",
       " 97.78480271373601,\n",
       " 94.41309090626105,\n",
       " 96.17917348212178,\n",
       " 92.38637355540314,\n",
       " 96.94437759593806,\n",
       " 94.14743089264283,\n",
       " 94.29578529286695,\n",
       " 96.37266518896823,\n",
       " 99.33089786594613,\n",
       " 90.6089878720507,\n",
       " 98.17042933742255,\n",
       " 92.815653508237,\n",
       " 94.30943644396109,\n",
       " 90.20803202547641,\n",
       " 92.90040825067737,\n",
       " 94.89463371081115,\n",
       " 90.53426617466575,\n",
       " 94.77937000663604,\n",
       " 99.23497515298996,\n",
       " 91.00495486831745,\n",
       " 97.54868040725329,\n",
       " 93.96974077180916,\n",
       " 96.43209778183862,\n",
       " 92.50182115502727,\n",
       " 91.5251554415019,\n",
       " 91.5345131550702,\n",
       " 90.67092451366008,\n",
       " 91.77771414864074,\n",
       " 96.5240468292431,\n",
       " 98.76122298897457,\n",
       " 98.06067035529212,\n",
       " 96.7369824477237,\n",
       " 91.05327988317892,\n",
       " 92.78440803335401,\n",
       " 92.98619883047101,\n",
       " 92.40168949837954,\n",
       " 95.01843564590085,\n",
       " 95.22436999956278,\n",
       " 93.44858207556933,\n",
       " 93.4180964425529,\n",
       " 98.20986959755032,\n",
       " 97.80443971157246,\n",
       " 92.96649857454578,\n",
       " 99.09578344842494,\n",
       " 99.50409029859122,\n",
       " 93.15226692582564,\n",
       " 93.42435446589495,\n",
       " 94.9246620060092,\n",
       " 99.38156606831254,\n",
       " 98.70534882107408,\n",
       " 90.97786942038813,\n",
       " 93.80216942621963,\n",
       " 95.40329763223686,\n",
       " 94.89964521384451,\n",
       " 91.85719870196688,\n",
       " 98.52349505345565,\n",
       " 96.65350794083615,\n",
       " 98.30669461774532,\n",
       " 98.52295016436071,\n",
       " 97.38350362145584,\n",
       " 94.26265857148523,\n",
       " 95.97974611805206,\n",
       " 91.53357567245226,\n",
       " 96.61995553159171,\n",
       " 91.71591192879508,\n",
       " 96.82687180279314,\n",
       " 92.20369917313499,\n",
       " 91.1216052561733,\n",
       " 91.58102773603497,\n",
       " 90.0188922980602,\n",
       " 97.08010350991808,\n",
       " 97.21993883989715,\n",
       " 95.01771174144761,\n",
       " 91.54319507930387,\n",
       " 90.79579722028832,\n",
       " 90.18157231149007,\n",
       " 91.58550197074558,\n",
       " 98.85663496112906,\n",
       " 92.31975200688963,\n",
       " 93.42284234437459,\n",
       " 91.00449718375471,\n",
       " 92.13342259541827,\n",
       " 95.06068136058339,\n",
       " 95.95149529260766,\n",
       " 92.3568720213323,\n",
       " 90.54911044722189,\n",
       " 93.78539076055411,\n",
       " 92.24844793810028,\n",
       " 92.1061747415815,\n",
       " 99.57800533009838,\n",
       " 96.03794822090971,\n",
       " 95.43855814841376,\n",
       " 94.0301945808229,\n",
       " 94.16075361605465,\n",
       " 95.27807302050333,\n",
       " 96.85217760627903,\n",
       " 96.91052644995032,\n",
       " 91.64289324841248,\n",
       " 94.8694568111629,\n",
       " 93.99238681426986,\n",
       " 98.64512759181834,\n",
       " 96.19104763998841,\n",
       " 95.19377712794419,\n",
       " 91.10090235620689,\n",
       " 95.93011816870361,\n",
       " 91.11827011515021,\n",
       " 93.25502355861,\n",
       " 91.36473508246132,\n",
       " 96.80773582664287,\n",
       " 91.97717968696955,\n",
       " 93.79355803182914,\n",
       " 98.90684294309817,\n",
       " 91.95842289469972,\n",
       " 94.94637644143178,\n",
       " 94.06484160298277,\n",
       " 90.17202444101214,\n",
       " 97.84015484066016,\n",
       " 90.65711135649742,\n",
       " 94.61134774582918,\n",
       " 95.47677503916113,\n",
       " 96.16072457460922,\n",
       " 96.77263459907275,\n",
       " 92.4339725403727,\n",
       " 90.0946729742901,\n",
       " 97.12823032997079,\n",
       " 95.22540123027588,\n",
       " 98.28814922776098,\n",
       " 98.13573504006344,\n",
       " 98.36282620748011,\n",
       " 94.7194622488897,\n",
       " 93.21987314716193,\n",
       " 97.82778635315633,\n",
       " 94.38274314681964,\n",
       " 91.75639958401447,\n",
       " 98.48361349731947,\n",
       " 98.77026856842744,\n",
       " 94.53218010021521,\n",
       " 90.72068880830732,\n",
       " 93.75370903889524,\n",
       " 97.95133879868028,\n",
       " 98.97042290724828,\n",
       " 91.95024940108567,\n",
       " 90.63731962530085,\n",
       " 98.7160716468517,\n",
       " 93.76930611106093,\n",
       " 95.28666600689391,\n",
       " 99.66410514227218,\n",
       " 90.63341503185333,\n",
       " 96.36246801924693,\n",
       " 90.70545111994568,\n",
       " 93.6317251763756,\n",
       " 97.95419635746822,\n",
       " 94.20568483202811,\n",
       " 99.970317541541,\n",
       " 95.4604900258642,\n",
       " 93.0975138962637,\n",
       " 92.11158413333705,\n",
       " 93.38593028786283,\n",
       " 93.6027691660483,\n",
       " 90.65393807085142,\n",
       " 93.57359121700891,\n",
       " 94.51155292086285,\n",
       " 97.12100668124347,\n",
       " 96.4479059134982,\n",
       " 96.9781393177289,\n",
       " 90.07934311869758,\n",
       " 92.08786906745993,\n",
       " 96.4982687875624,\n",
       " 94.70843242008392,\n",
       " 90.05039885436425,\n",
       " 97.96121555565058,\n",
       " 97.6347314831257,\n",
       " 95.35353412680678,\n",
       " 90.6324467294192,\n",
       " 97.570002155189,\n",
       " 95.70877958299289,\n",
       " 97.72890566435308,\n",
       " 97.41830419704665,\n",
       " 97.94581158255536,\n",
       " 95.04849806856338,\n",
       " 91.34050476173664,\n",
       " 96.59463175821328,\n",
       " 96.08018548298182,\n",
       " 97.3209080939358,\n",
       " 91.62539037110719,\n",
       " 91.86796268121213,\n",
       " 98.8519895338879,\n",
       " 97.40014680709898,\n",
       " 99.03775016344524,\n",
       " 97.48985368364804,\n",
       " 95.84442090693867,\n",
       " 96.42152188935341,\n",
       " 98.83864251723294,\n",
       " 95.6593122261782,\n",
       " 96.20090319596575,\n",
       " 91.49608073042705,\n",
       " 94.60802037709043,\n",
       " 97.05614635626627,\n",
       " 92.60622210363941,\n",
       " 91.93844482104114,\n",
       " 93.02559075149658,\n",
       " 92.31939976200593,\n",
       " 92.06114506787843,\n",
       " 94.12015156657905,\n",
       " 99.03368098913482,\n",
       " 94.93891987915302,\n",
       " 91.79970265377642,\n",
       " 90.73284732679946,\n",
       " 96.84938509232956,\n",
       " 93.70406257338642,\n",
       " 98.14031129903299,\n",
       " 96.47617290893326,\n",
       " 97.87606937208659,\n",
       " 92.61601225967975,\n",
       " 96.81064559794237,\n",
       " 92.53911103952308,\n",
       " 99.3600786313482,\n",
       " 96.24128203289462,\n",
       " 93.13037572445059,\n",
       " 92.59226243256242,\n",
       " 91.82878345898183,\n",
       " 96.83392521267254,\n",
       " 92.09803984153619,\n",
       " 95.82572042795182,\n",
       " 92.54469835187462,\n",
       " 96.50116296308107,\n",
       " 98.0690811820725,\n",
       " 97.68839901839888,\n",
       " 97.51179635054885,\n",
       " 91.79778784568425,\n",
       " 90.84202000279593,\n",
       " 96.87788610856317,\n",
       " 93.56149519438061,\n",
       " 94.19450925187489,\n",
       " 90.29585481822812,\n",
       " 92.49538754331476,\n",
       " 90.31990189145017,\n",
       " 98.73516223963738,\n",
       " 92.33784419132449,\n",
       " 95.44317096288819,\n",
       " 90.37037935067288,\n",
       " 96.5507768896865,\n",
       " 93.11581100417692,\n",
       " 98.930234420062,\n",
       " 98.82812800886192,\n",
       " 93.13801800796867,\n",
       " 98.96194426677532,\n",
       " 99.95952524309118,\n",
       " 98.17738468698221,\n",
       " 98.3788410734148,\n",
       " 92.39245866803223,\n",
       " 95.63810537744544,\n",
       " 90.64001553453302,\n",
       " 90.90456983198872,\n",
       " 99.98833318390535,\n",
       " 93.15125315875764,\n",
       " 97.38173945176906,\n",
       " 97.98361945489881,\n",
       " 98.51525848025946,\n",
       " 99.97501474096629,\n",
       " 92.31935946636948,\n",
       " 90.38343904903746,\n",
       " 93.98479564222129,\n",
       " 91.24197439696749,\n",
       " 90.21290371790526,\n",
       " 93.48345106711864,\n",
       " 97.74721785356486,\n",
       " 95.53214234458221,\n",
       " 93.01531160913726,\n",
       " 96.42364787557572,\n",
       " 92.22721176341409,\n",
       " 90.13647466125299,\n",
       " 97.54778273329617,\n",
       " 96.11328649480812,\n",
       " 97.5267079256409,\n",
       " 90.36998607699394,\n",
       " 98.29852539631182,\n",
       " 96.07059350384559,\n",
       " 95.50411112606238,\n",
       " 96.1221067484192,\n",
       " 98.58181405576943,\n",
       " 95.74128477770181,\n",
       " 95.6809196542268,\n",
       " 99.90184332342115,\n",
       " 97.46946514432864,\n",
       " 94.29328458026882,\n",
       " 96.96415586326303,\n",
       " 93.76758246295633,\n",
       " 92.19667869463511,\n",
       " 95.84135686281024,\n",
       " 99.24617847660186,\n",
       " 99.25619038674787,\n",
       " 93.30120160764228,\n",
       " 95.14538024950565,\n",
       " 92.0293674693568,\n",
       " 99.95397973696797,\n",
       " 99.80125506344426,\n",
       " 96.37455997252212,\n",
       " 97.96038384489654,\n",
       " 97.04288479310453,\n",
       " 95.80406595114577,\n",
       " 90.50729387863899,\n",
       " 94.41725816291486,\n",
       " 96.63157035406415,\n",
       " 96.65952483284424,\n",
       " 93.60858148479731,\n",
       " 99.38754695144094,\n",
       " 91.6010279286576,\n",
       " 94.87192267321394,\n",
       " 96.79560228973884,\n",
       " 96.85938216568712,\n",
       " 96.36572660834656,\n",
       " 92.64863117332045,\n",
       " 91.49347652729561,\n",
       " 96.23688181926532,\n",
       " 95.86610231494714,\n",
       " 91.71690487561322,\n",
       " 96.9410919286999,\n",
       " 94.42131348331819,\n",
       " 96.56006699415002,\n",
       " 98.30108823584094,\n",
       " 91.62391744603504,\n",
       " 90.1817234015085,\n",
       " 97.69948820326451,\n",
       " 95.97115767486359,\n",
       " 96.88493683174912,\n",
       " 98.30924884736405,\n",
       " 97.94383913960013,\n",
       " 99.58909917001243,\n",
       " 95.22819525540376,\n",
       " 94.75340898832422,\n",
       " 93.89270676345976,\n",
       " 91.46923057479675,\n",
       " 95.59684941995766,\n",
       " 92.66546457599793,\n",
       " 99.17431951268128,\n",
       " 95.70350801831486,\n",
       " 95.80085573487551,\n",
       " 93.42927338043862,\n",
       " 90.49433890951909,\n",
       " 90.30151248312033,\n",
       " 94.10512226805206,\n",
       " 90.80546964494032,\n",
       " 95.94233927764753,\n",
       " 98.75375619594172,\n",
       " 98.77396231314725,\n",
       " 96.47144275834789,\n",
       " 92.02544961247357,\n",
       " 98.5668956671635,\n",
       " 98.80704734332464,\n",
       " 91.88420963614708,\n",
       " 97.27142463709006,\n",
       " 92.7583308325775,\n",
       " 97.9420784316072,\n",
       " 99.97090260506783,\n",
       " 90.28517082543708,\n",
       " 98.92446713324117,\n",
       " 96.10202085416638,\n",
       " 99.71995077447858,\n",
       " 94.51923066363454,\n",
       " 98.40492953044158,\n",
       " 90.59359739041253,\n",
       " 93.23608677770922,\n",
       " 90.63772954951418,\n",
       " 99.74055113473733,\n",
       " 98.09010180797681,\n",
       " 98.45843764177428,\n",
       " 99.34852980826027,\n",
       " 90.81061803774439,\n",
       " 93.7316986612644,\n",
       " 90.67611147382043,\n",
       " 92.02119585178158,\n",
       " 92.20144167911447,\n",
       " 94.56004674359676,\n",
       " 92.58299808299975,\n",
       " 90.96263117361656,\n",
       " 91.60232173815822,\n",
       " 91.40742391823392,\n",
       " 99.71673770135922,\n",
       " 97.49578247429616,\n",
       " 99.66090935097965,\n",
       " 94.26792130685162,\n",
       " 92.6787041143847,\n",
       " 97.89722983121689,\n",
       " 93.1474714144955,\n",
       " 92.88413507004147,\n",
       " 92.23195200544626,\n",
       " 91.23811289618722,\n",
       " 92.46505142436267,\n",
       " 93.43316260745435,\n",
       " 96.62350626086759,\n",
       " 90.59522743726514,\n",
       " 92.01842143948454,\n",
       " 98.00797160305822,\n",
       " 91.40401816945706,\n",
       " 93.30847499204175,\n",
       " 98.58415808642795,\n",
       " 91.48112629462439,\n",
       " 90.78797709525008,\n",
       " 94.71390706789599,\n",
       " 92.91385243788864,\n",
       " 95.50424207543425,\n",
       " 97.95409671603984,\n",
       " 91.30994476983744,\n",
       " 95.67838638175544,\n",
       " 94.92340791953983,\n",
       " 91.37664179049555,\n",
       " 96.1158728560422,\n",
       " 92.63254760040405,\n",
       " 94.74680714225146,\n",
       " 90.78353393557023,\n",
       " 94.46521614194164,\n",
       " 92.95344899103434,\n",
       " 98.14600126255948,\n",
       " 90.54192276229958,\n",
       " 94.05825728722016,\n",
       " 94.45374479222664,\n",
       " 97.14422057859147,\n",
       " 95.61377554902374,\n",
       " 96.55448942407332,\n",
       " 97.67835449224617,\n",
       " 98.55865175291108,\n",
       " 93.02606795960652,\n",
       " 95.24914332499189,\n",
       " 98.32948100489791,\n",
       " 99.88891831331925,\n",
       " 98.84271838537268,\n",
       " 93.59225671619525,\n",
       " 91.86981450916242,\n",
       " 94.76236345189331,\n",
       " 97.31463569559467,\n",
       " 94.7940839404239,\n",
       " 94.70036507421847,\n",
       " 98.31003031455063,\n",
       " 93.49307401578332,\n",
       " 98.53364669173912,\n",
       " 93.94136714993465,\n",
       " 93.16689731719624,\n",
       " 94.40955454949088,\n",
       " 97.52819103756005,\n",
       " 91.2027736056002,\n",
       " 91.88194074465599,\n",
       " 99.48972652035673,\n",
       " 91.67956962926998,\n",
       " 95.54771089921057,\n",
       " 95.66442944883383,\n",
       " 94.76467906487427,\n",
       " 96.32620134168502,\n",
       " 92.20591962453885,\n",
       " 95.3961719985529,\n",
       " 93.59921871784414,\n",
       " 96.49809565437812,\n",
       " 91.34915710456478,\n",
       " 95.57942806971222,\n",
       " 91.77407501606089,\n",
       " 92.68138535210213,\n",
       " 92.0979834382856,\n",
       " 91.7505565722623,\n",
       " 98.18043919361202,\n",
       " 92.75039562585889,\n",
       " 99.23511267365777,\n",
       " 99.68364577811118,\n",
       " 95.58328560633109,\n",
       " 91.3662671862825,\n",
       " 93.62264297854607,\n",
       " 97.89144424840669,\n",
       " 93.55206693838639,\n",
       " 90.82893019308764,\n",
       " 95.43761764371278,\n",
       " 98.38142727491548,\n",
       " 97.87282040619691,\n",
       " 91.67601033013077,\n",
       " 96.61098943557815,\n",
       " 92.11539764115129,\n",
       " 92.09310983583416,\n",
       " 98.68197672096464,\n",
       " 92.39888578164485,\n",
       " 92.53116868385938,\n",
       " 90.00619595614558,\n",
       " 98.64660415693287,\n",
       " 97.83847147511192,\n",
       " 96.14960563040205,\n",
       " 97.40310199110147,\n",
       " 91.45340300581363,\n",
       " 94.45267171156205,\n",
       " 93.39280668892434,\n",
       " 90.89363834158874,\n",
       " 94.729800029165,\n",
       " 99.16838606887931,\n",
       " 90.3806612032577,\n",
       " 92.80663568574256,\n",
       " 91.98967020723667,\n",
       " 92.28678202241325,\n",
       " 99.03990038773522,\n",
       " 94.54970426162046,\n",
       " 94.53207708241742,\n",
       " 97.50896399774535,\n",
       " 91.47695188730391,\n",
       " 94.7411275837001,\n",
       " 94.17108057807042,\n",
       " 95.84195075878951,\n",
       " 99.99534004721842,\n",
       " 97.60016681455355,\n",
       " 93.85293219480674,\n",
       " 98.1994963621312,\n",
       " 91.63337777669112,\n",
       " 90.2893343583198,\n",
       " 91.95970171751259,\n",
       " 93.28651908344163,\n",
       " 94.97414742725849,\n",
       " 96.02609656776949,\n",
       " 99.0625588615029,\n",
       " 94.96687548444228,\n",
       " 94.88108120980816,\n",
       " 90.4776898892045,\n",
       " 90.3316565191382,\n",
       " 95.38107353251547,\n",
       " 94.2524098755264,\n",
       " 98.31987063235445,\n",
       " 91.53660566871426,\n",
       " 90.23710422483016,\n",
       " 94.36031817587613,\n",
       " 92.28028608891177,\n",
       " 90.4716974368292,\n",
       " 97.14004687586154,\n",
       " 91.06441983575374,\n",
       " 95.95722569873506,\n",
       " 92.70488953126144,\n",
       " 91.6602955654141,\n",
       " 93.67398287554667,\n",
       " 97.92975251672857,\n",
       " 93.79587249374727,\n",
       " 97.40827322127284,\n",
       " 91.19781799211533,\n",
       " 97.63433900346035,\n",
       " 92.27498511631043,\n",
       " 96.65811450864445,\n",
       " 95.52812499530931,\n",
       " 99.25624473739546,\n",
       " 93.74873491786494,\n",
       " 90.62959984022962,\n",
       " 90.18182428524439,\n",
       " 98.19231117523219,\n",
       " 95.11589426384955,\n",
       " 97.65977780019169,\n",
       " 92.25060788432219,\n",
       " 93.33245200963755,\n",
       " 90.28591153657892,\n",
       " 99.5951030730301,\n",
       " 96.56115878317988,\n",
       " 99.29871591432116,\n",
       " 92.55309767283164,\n",
       " 95.98901825189408,\n",
       " 96.66938715154251,\n",
       " 93.06208680827189,\n",
       " 98.41240829485562,\n",
       " 99.44483720443651,\n",
       " 98.79415685025705,\n",
       " 97.288242864444,\n",
       " 92.66987920142726,\n",
       " 92.7104209159572,\n",
       " 99.61470661393179,\n",
       " 90.097575037178,\n",
       " 97.0525966203756,\n",
       " 96.95239939771456,\n",
       " 96.10261832660646,\n",
       " 99.89584057595184,\n",
       " 93.00650607531442,\n",
       " 93.28219757269932,\n",
       " 90.7516601399166,\n",
       " 94.2982951364243,\n",
       " 92.50973457835387,\n",
       " 93.31576553202609,\n",
       " 98.27214153602183,\n",
       " 99.32256650421702,\n",
       " 91.77854930050971,\n",
       " 93.60252910264279,\n",
       " 99.25086800886939,\n",
       " 90.58800794230415,\n",
       " 90.88007890438296,\n",
       " 91.55697104507131,\n",
       " 95.82480741047706,\n",
       " 91.4382157403769,\n",
       " 99.66421612802546,\n",
       " 94.33721374867491,\n",
       " 94.88350360309265,\n",
       " 92.3695842138642,\n",
       " 94.57851020321696,\n",
       " 96.50060058321628,\n",
       " 97.4124553520072,\n",
       " 97.44007759159754,\n",
       " 95.65008417887863,\n",
       " 98.99738964490767,\n",
       " 98.0945866042526,\n",
       " 97.47273034093783,\n",
       " 90.52882395781083,\n",
       " 90.0652435451751,\n",
       " 92.03723514506558,\n",
       " 96.5169710162817,\n",
       " 93.98485476768332,\n",
       " 93.89784174267126,\n",
       " 98.79136488830535,\n",
       " 98.90567654667451,\n",
       " 99.04762994309057,\n",
       " 93.02609324486492,\n",
       " 96.79735476572752,\n",
       " 92.61295941807191,\n",
       " 91.82571889400924,\n",
       " 91.76790715511177,\n",
       " 93.30319071131521,\n",
       " 94.17080678979855,\n",
       " 98.23464008136673,\n",
       " 91.14407149023863,\n",
       " 97.24580415013796,\n",
       " 95.18325506367019,\n",
       " 92.7722662550516,\n",
       " 94.79658599323554,\n",
       " 92.88960280971318,\n",
       " 95.8335825903311,\n",
       " 94.20937413647798,\n",
       " 91.57254701205962,\n",
       " 91.11231016494389,\n",
       " 95.33622075198262,\n",
       " 98.97708640127625,\n",
       " 93.32137950350536,\n",
       " 97.22607527192677,\n",
       " 96.4675391924473,\n",
       " 99.2814530299374,\n",
       " 98.13152332696993,\n",
       " 95.53641584793694,\n",
       " 96.45096480144694,\n",
       " 98.9351051546213,\n",
       " 93.8692799391741,\n",
       " 93.15277889281711,\n",
       " 90.10257392910461,\n",
       " 98.18913009381079,\n",
       " 97.9249540149043,\n",
       " 90.9959547489096,\n",
       " 95.639667066005,\n",
       " 94.5089955812173,\n",
       " 91.13272148665939,\n",
       " 99.79547527572767,\n",
       " 92.05904270561948,\n",
       " 90.6398079788818,\n",
       " 95.82111779638913,\n",
       " 97.28618975535451,\n",
       " 90.3081696401058,\n",
       " 96.4681112552219,\n",
       " 95.19164895236358,\n",
       " 90.98455118570824,\n",
       " 91.65090740208038,\n",
       " 95.5501972621846,\n",
       " 92.85452873088387,\n",
       " 99.35219867489829,\n",
       " 98.10835813535475,\n",
       " 99.82701303568297,\n",
       " 92.49630740941852,\n",
       " 99.68151487720849,\n",
       " 94.18347044674297,\n",
       " 93.35640027836929,\n",
       " 90.47870529900545,\n",
       " 90.50173710998813,\n",
       " 96.80709945765012,\n",
       " 94.44874489732891,\n",
       " 92.1738166222574,\n",
       " 96.01437294164079,\n",
       " 92.42993814407879,\n",
       " 95.64747046800687,\n",
       " 93.4714543995305,\n",
       " 98.15811114439362,\n",
       " 98.12857818595323,\n",
       " 94.64220842705335,\n",
       " 93.38886807063737,\n",
       " 93.50446203472828,\n",
       " 97.97380707757237,\n",
       " 93.16637095046177,\n",
       " 92.0043882008739,\n",
       " 90.8066460852806,\n",
       " 94.53599847455693,\n",
       " 94.68990141755762,\n",
       " 98.33777593778329,\n",
       " 92.12252582857923,\n",
       " 93.68621944324556,\n",
       " 98.00159125560602,\n",
       " 98.15884940114579,\n",
       " 93.72195670504,\n",
       " 94.46063757944422,\n",
       " 92.92233420570572,\n",
       " 99.29191355063382,\n",
       " 91.13209420607511,\n",
       " 99.30441100215742,\n",
       " 96.72406097409124,\n",
       " 95.18443080386646,\n",
       " 90.31022121937642,\n",
       " 94.62471788430224,\n",
       " 93.950704164689,\n",
       " 91.54180630965318,\n",
       " 96.44465539893102,\n",
       " 99.69650594953984,\n",
       " 95.49197125851455,\n",
       " 97.03933813790225,\n",
       " 90.65142395124396,\n",
       " 94.04893955797162,\n",
       " 91.11015421315274,\n",
       " 96.00249258476101,\n",
       " 99.35394084136493,\n",
       " 96.49921884814913,\n",
       " 90.73123538112489,\n",
       " 93.42672923823292,\n",
       " 95.37608330758307,\n",
       " 93.90577138586579,\n",
       " 98.2625357704172,\n",
       " 98.06630460311894,\n",
       " 95.99273409603127,\n",
       " 93.61188875266832,\n",
       " 92.45407570214175,\n",
       " 91.00956793131061,\n",
       " 93.42900782195551,\n",
       " 94.00441411135448,\n",
       " 96.64212494620449,\n",
       " 96.46244183504002,\n",
       " 90.66725803710432,\n",
       " 93.82716274761822,\n",
       " 91.74032075543046,\n",
       " 91.49931974968266,\n",
       " 98.19690047108807,\n",
       " 90.3983377201291,\n",
       " 94.06396168118849,\n",
       " 91.6091991681404,\n",
       " 98.73666486172495,\n",
       " 95.44018785106839,\n",
       " 92.22101688913523,\n",
       " 94.89259455590239,\n",
       " 97.21056999758765,\n",
       " 99.55970135147831,\n",
       " 92.1148322184649,\n",
       " 98.8079669657903,\n",
       " 99.31021477448478,\n",
       " 99.12326845071331,\n",
       " 96.2315488590758,\n",
       " 96.17485098802237,\n",
       " 93.9057998512948,\n",
       " 97.42498518797915,\n",
       " 95.18164152428692,\n",
       " 96.65857219002709,\n",
       " 94.15592795669266,\n",
       " 97.21067348786735,\n",
       " 98.16761890235665,\n",
       " 91.39934113421751,\n",
       " 98.25514095968767,\n",
       " 95.2719650411019,\n",
       " 98.37671167427783,\n",
       " 94.18414360608594,\n",
       " 93.66696552560936,\n",
       " 99.10906802824334,\n",
       " 92.41349279220525,\n",
       " 98.37201038092907,\n",
       " 94.70874379391283,\n",
       " 95.0093237246611,\n",
       " 92.97308545256756,\n",
       " 95.6012305733976,\n",
       " 93.13723863677603,\n",
       " 90.37314151014155,\n",
       " 92.61652520312737,\n",
       " 90.06050926908601,\n",
       " 99.77220476830588,\n",
       " 99.63976125866125,\n",
       " 93.82613446517948,\n",
       " 97.17866574290653,\n",
       " 93.33955557709136,\n",
       " 96.59397719677409,\n",
       " 97.97087052956223,\n",
       " 99.44059159725322,\n",
       " 93.87613889860378,\n",
       " 97.74268377064634,\n",
       " 92.55731081300033,\n",
       " 99.8981925132699,\n",
       " 90.24469086882333,\n",
       " 95.90880351656669,\n",
       " 96.47612425025423,\n",
       " 96.76665918246174,\n",
       " 91.14883935666772,\n",
       " 99.35443938299038,\n",
       " 91.73456409547678,\n",
       " 96.10326496138165,\n",
       " 92.13647823174222,\n",
       " 92.96082338221224,\n",
       " 95.33383878443915,\n",
       " 94.04424697684817,\n",
       " 91.53222739812314,\n",
       " 91.63557941581793,\n",
       " 94.05367414288601,\n",
       " 97.4751423673717,\n",
       " 98.9307082651112,\n",
       " 90.80128197790745,\n",
       " 93.8058574981491,\n",
       " 90.95561084694829,\n",
       " 90.15747491650791,\n",
       " 96.4986184781683,\n",
       " 95.89789715703213,\n",
       " 91.55917167320109,\n",
       " 92.24465224462853,\n",
       " 90.22508850630118,\n",
       " 98.2760286656667,\n",
       " 99.73369403998727,\n",
       " 91.29369772162956,\n",
       " 92.22193585054812,\n",
       " 98.62312282999603,\n",
       " 99.22821247646982,\n",
       " 94.06836712670575,\n",
       " 90.48130729203916,\n",
       " 90.37430716638916,\n",
       " 95.61037355700056,\n",
       " 93.80379387311196,\n",
       " 90.27210562327663,\n",
       " 95.70412903533766,\n",
       " 90.1072395374439,\n",
       " 97.78464746720687,\n",
       " 92.95314886260786,\n",
       " 90.38255510615045,\n",
       " 95.75556665132305,\n",
       " 93.85113955909817,\n",
       " 99.7243404447565,\n",
       " 95.31114230505075,\n",
       " 92.64815254426874,\n",
       " 96.98501819104642,\n",
       " 92.61158184166705,\n",
       " 98.99617468009843,\n",
       " 93.62435666119256,\n",
       " 95.36809663002445,\n",
       " 90.48083665611806,\n",
       " 94.1327503431835,\n",
       " 98.2492799612384,\n",
       " 97.97312867741876,\n",
       " 92.15242406038084,\n",
       " 92.16951885780048,\n",
       " 98.09352687941819,\n",
       " 99.26935643913347,\n",
       " 90.9088127263736,\n",
       " 94.37045813190626,\n",
       " 93.25745737468881,\n",
       " 98.6468060119192,\n",
       " 90.7955157533345,\n",
       " 92.02442293528962,\n",
       " 97.42570839629246,\n",
       " 90.48770785814665,\n",
       " 94.79374745995187,\n",
       " 94.29142907030396,\n",
       " 93.22746349573623,\n",
       " 93.8203715313373,\n",
       " 95.16806975812727,\n",
       " 91.54323903496545]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperparameter tuning used randomizedSearchCV\n",
    "# digunakan untuk sebuah randomisasi dalam format yang ter scale \n",
    "# https://docs.scipy.org/doc/scipy-1.2.1/reference/generated/scipy.stats.reciprocal.html\n",
    "from scipy.stats import reciprocal\n",
    "# sama kayak reciprocal cuma format standarnya berbentuk exponensial\n",
    "#https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.expon.html \n",
    "from scipy.stats import expon\n",
    "# reciprocal(3e-4, 3e-2).rvs(1000).tolist()\n",
    "reciprocal(90,100).rvs(1000).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[26.97244906536377,\n",
       " 42.73950510194796,\n",
       " 38.55574242691,\n",
       " 13.34235220819778,\n",
       " 13.232726846354243,\n",
       " 16.244260013969708,\n",
       " 18.959355481473118,\n",
       " 20.50037007107788,\n",
       " 32.769770663103024,\n",
       " 11.167003390427585,\n",
       " 10.705761370840593,\n",
       " 19.948666148259477,\n",
       " 33.89125715066162,\n",
       " 14.299424294130878,\n",
       " 30.516782380760525,\n",
       " 16.017457474444655,\n",
       " 53.44624912940855,\n",
       " 55.20775019978245,\n",
       " 17.05614023175558,\n",
       " 15.227133048625124,\n",
       " 20.59690620306006,\n",
       " 15.50273089236741,\n",
       " 32.31825219635884,\n",
       " 44.981435983227506,\n",
       " 32.36153787377161,\n",
       " 44.785288930532,\n",
       " 20.11650168683739,\n",
       " 13.399568252751191,\n",
       " 36.784471381437854,\n",
       " 18.940575837911872,\n",
       " 32.25042088402471,\n",
       " 16.311864928669912,\n",
       " 11.694381250078765,\n",
       " 108.07636461135083,\n",
       " 13.396831327292032,\n",
       " 99.17123290037557,\n",
       " 85.69013434215007,\n",
       " 41.57993018123753,\n",
       " 31.542278591820526,\n",
       " 27.245856850986517,\n",
       " 50.21347135269835,\n",
       " 16.83402428388424,\n",
       " 22.610254889028177,\n",
       " 29.319599393525102,\n",
       " 20.59305072409802,\n",
       " 21.15437765711922,\n",
       " 18.018050675033187,\n",
       " 26.612897333471885,\n",
       " 48.01915051205327,\n",
       " 14.501117022047719,\n",
       " 64.49366698870503,\n",
       " 33.3648850275648,\n",
       " 44.66300626820313,\n",
       " 26.24720593423381,\n",
       " 40.23866660767406,\n",
       " 10.326661170066814,\n",
       " 44.116820511199315,\n",
       " 10.819332997258497,\n",
       " 54.12975470970122,\n",
       " 106.47335901062861,\n",
       " 16.96471203091597,\n",
       " 14.72251301294582,\n",
       " 38.99434381334317,\n",
       " 15.834507134943143,\n",
       " 50.133100058851944,\n",
       " 12.170478995479652,\n",
       " 12.692472838332876,\n",
       " 87.40951837360713,\n",
       " 32.40727685189675,\n",
       " 47.531610677507516,\n",
       " 17.840991325684687,\n",
       " 32.57011865963344,\n",
       " 28.039958612113686,\n",
       " 28.48394412272716,\n",
       " 32.947594012803954,\n",
       " 27.130231612243637,\n",
       " 21.209506069855543,\n",
       " 16.457142331581302,\n",
       " 39.270535772460015,\n",
       " 15.130708881981345,\n",
       " 33.57613005640039,\n",
       " 15.314049992473073,\n",
       " 29.634201881003825,\n",
       " 37.49350388712701,\n",
       " 14.936277136002179,\n",
       " 11.236301746090163,\n",
       " 12.807638779532745,\n",
       " 28.62793344864332,\n",
       " 47.86872306742176,\n",
       " 10.920812199153051,\n",
       " 36.496555499058026,\n",
       " 18.350307818644346,\n",
       " 23.022008132675516,\n",
       " 62.861983695115065,\n",
       " 18.068488975484605,\n",
       " 22.522370270944936,\n",
       " 10.275537633552316,\n",
       " 11.70241719389703,\n",
       " 15.984072929120996,\n",
       " 10.564949409174446,\n",
       " 29.959525926911294,\n",
       " 21.116402499791437,\n",
       " 25.887887914672515,\n",
       " 13.838886612122792,\n",
       " 17.017607567778857,\n",
       " 31.80243718297424,\n",
       " 77.22188171799013,\n",
       " 11.034641994069595,\n",
       " 54.215497136202366,\n",
       " 27.202045828044398,\n",
       " 26.59857145458272,\n",
       " 23.874088123936772,\n",
       " 11.438387688981225,\n",
       " 11.883507217295122,\n",
       " 28.37146698423506,\n",
       " 18.339377613044,\n",
       " 59.84299231569099,\n",
       " 20.437412495474923,\n",
       " 13.092916751013874,\n",
       " 35.08560071835153,\n",
       " 16.94449253911396,\n",
       " 24.90862172380171,\n",
       " 33.93549448913991,\n",
       " 56.10459397808589,\n",
       " 41.422736555736165,\n",
       " 32.562535832352,\n",
       " 32.76577269358253,\n",
       " 68.34753448618278,\n",
       " 17.01454185047362,\n",
       " 10.022254260635657,\n",
       " 16.347836286486626,\n",
       " 14.915232946991512,\n",
       " 31.684402201180795,\n",
       " 30.08668554180705,\n",
       " 28.001527100201276,\n",
       " 10.327536283486811,\n",
       " 36.09635612045055,\n",
       " 17.817355760289693,\n",
       " 31.875837849763023,\n",
       " 26.264686655703755,\n",
       " 18.393235880046685,\n",
       " 12.890720547504648,\n",
       " 11.983568786178013,\n",
       " 45.618566739240606,\n",
       " 60.11791791832372,\n",
       " 30.984334542304595,\n",
       " 12.183669942528482,\n",
       " 20.284200785166,\n",
       " 36.108242588209805,\n",
       " 40.26627800581139,\n",
       " 12.51622249557581,\n",
       " 10.000614386344315,\n",
       " 34.905415317052366,\n",
       " 18.819654414957633,\n",
       " 15.87350207333354,\n",
       " 10.259627285696707,\n",
       " 25.54359771360499,\n",
       " 48.09642264502558,\n",
       " 73.22707139547447,\n",
       " 26.682464921899893,\n",
       " 24.449926798445517,\n",
       " 11.777856781818603,\n",
       " 25.920118855067273,\n",
       " 19.560041158162004,\n",
       " 28.685799050783295,\n",
       " 19.84421088522358,\n",
       " 15.496610133267895,\n",
       " 11.995639322098421,\n",
       " 17.564347776251168,\n",
       " 12.032789707668261,\n",
       " 13.893477490157807,\n",
       " 97.38008957111781,\n",
       " 21.743465110160976,\n",
       " 25.18026034804201,\n",
       " 51.36900792250696,\n",
       " 118.38215058975362,\n",
       " 27.479690355602852,\n",
       " 43.47673644440077,\n",
       " 17.918446443525973,\n",
       " 17.298288982403296,\n",
       " 20.306840179089843,\n",
       " 32.32578800510939,\n",
       " 32.92613748712872,\n",
       " 17.57372082914727,\n",
       " 12.85916917101889,\n",
       " 30.014994096702523,\n",
       " 12.746792401703987,\n",
       " 27.27835455255585,\n",
       " 33.64566975087965,\n",
       " 34.12026411689793,\n",
       " 38.005603011180256,\n",
       " 51.24923775796897,\n",
       " 23.861303357053124,\n",
       " 36.20513340648222,\n",
       " 29.308187606479258,\n",
       " 14.131627779968365,\n",
       " 10.520507726600046,\n",
       " 16.696630465509614,\n",
       " 21.705011099140666,\n",
       " 29.199938907486235,\n",
       " 47.93697783312682,\n",
       " 14.369233486150886,\n",
       " 12.68319842839829,\n",
       " 76.19787569969955,\n",
       " 12.286782042154973,\n",
       " 23.02057762423535,\n",
       " 27.61672129647883,\n",
       " 25.59042267272652,\n",
       " 11.799197817926107,\n",
       " 11.174350751739247,\n",
       " 12.22943698102623,\n",
       " 27.617877361877664,\n",
       " 25.7203817632581,\n",
       " 15.32469549772259,\n",
       " 30.30974688316776,\n",
       " 44.303470196931364,\n",
       " 10.849361968928685,\n",
       " 23.78828088975233,\n",
       " 33.37921949801984,\n",
       " 15.815388055631175,\n",
       " 17.356766328764053,\n",
       " 29.019605481555978,\n",
       " 55.642965776559066,\n",
       " 43.20666996077594,\n",
       " 27.499627639396607,\n",
       " 36.20309668425783,\n",
       " 19.077014893743982,\n",
       " 30.486737461892893,\n",
       " 22.55399112292359,\n",
       " 14.21284381088227,\n",
       " 34.19567804710759,\n",
       " 26.259554900166393,\n",
       " 18.882712809725028,\n",
       " 58.458746291371384,\n",
       " 10.423126866793854,\n",
       " 17.589315322466582,\n",
       " 11.171264661552424,\n",
       " 39.093803893528715,\n",
       " 34.22044883554692,\n",
       " 18.0499568580314,\n",
       " 32.51943757540525,\n",
       " 20.08154312617058,\n",
       " 38.23255447557461,\n",
       " 22.10945476927745,\n",
       " 20.630083437653234,\n",
       " 64.58442943068542,\n",
       " 15.777097862495095,\n",
       " 12.566836833282288,\n",
       " 27.5675286485524,\n",
       " 79.68406749536393,\n",
       " 19.491134416342682,\n",
       " 11.280890487561967,\n",
       " 18.289267445893216,\n",
       " 24.10162839568141,\n",
       " 13.532906103866885,\n",
       " 31.464869396002726,\n",
       " 131.9310709721432,\n",
       " 22.02009392424185,\n",
       " 18.761684168737986,\n",
       " 20.239998487558424,\n",
       " 12.446656418894698,\n",
       " 52.958451128251696,\n",
       " 20.716909028020183,\n",
       " 19.79676192988284,\n",
       " 31.557982281535516,\n",
       " 21.669124250727005,\n",
       " 30.89575174198686,\n",
       " 11.268303902214782,\n",
       " 43.61044622017622,\n",
       " 66.43690807170789,\n",
       " 30.92601726612184,\n",
       " 71.82985979456845,\n",
       " 13.284108531283636,\n",
       " 22.97939810195856,\n",
       " 28.41635596915969,\n",
       " 27.432880295003265,\n",
       " 20.265297897039154,\n",
       " 18.252344755406938,\n",
       " 12.712363312440896,\n",
       " 19.943137713213623,\n",
       " 13.550521035142783,\n",
       " 36.51269279309732,\n",
       " 14.684871978007232,\n",
       " 11.207005926238685,\n",
       " 26.075593052587685,\n",
       " 28.088712282005442,\n",
       " 51.100920737486746,\n",
       " 27.206792628377677,\n",
       " 18.47708881545453,\n",
       " 42.492237287587145,\n",
       " 25.54270525809384,\n",
       " 11.651992309234878,\n",
       " 26.68732946149207,\n",
       " 57.72336605846908,\n",
       " 20.093976138574234,\n",
       " 17.028796334648796,\n",
       " 13.097306032592568,\n",
       " 13.284419824499336,\n",
       " 21.33986061190636,\n",
       " 28.110964008861668,\n",
       " 11.6810150285514,\n",
       " 66.12492519935785,\n",
       " 38.140851890740976,\n",
       " 27.667949817834774,\n",
       " 45.25628944893761,\n",
       " 11.671692521610844,\n",
       " 22.957315783879725,\n",
       " 29.890079984897703,\n",
       " 45.281279818708484,\n",
       " 40.600482823770996,\n",
       " 16.48883684960762,\n",
       " 66.56377740795594,\n",
       " 12.655681243044528,\n",
       " 51.61022124764949,\n",
       " 80.76802420323533,\n",
       " 13.899712733624854,\n",
       " 35.60743669106824,\n",
       " 10.822389083014661,\n",
       " 20.39809513080023,\n",
       " 24.52511745116336,\n",
       " 27.380760022316515,\n",
       " 64.95784359757096,\n",
       " 32.31012506695971,\n",
       " 23.127741815146354,\n",
       " 43.1804466467409,\n",
       " 70.12891757812204,\n",
       " 10.446264958484853,\n",
       " 90.93541389859675,\n",
       " 11.801385805186085,\n",
       " 43.68180457557607,\n",
       " 16.579382183731813,\n",
       " 12.835411294480137,\n",
       " 21.591214844976143,\n",
       " 30.654051754475944,\n",
       " 19.59144429061903,\n",
       " 11.12221143576297,\n",
       " 28.298889752290727,\n",
       " 56.53822705668396,\n",
       " 19.66470738818356,\n",
       " 14.875416200094417,\n",
       " 21.688630447439305,\n",
       " 10.991426315803038,\n",
       " 44.35021812409876,\n",
       " 45.013211682372265,\n",
       " 27.734661089248767,\n",
       " 18.711360435225806,\n",
       " 42.16710641753563,\n",
       " 26.17955857887448,\n",
       " 44.966220481978525,\n",
       " 29.964054977277613,\n",
       " 40.687116524573895,\n",
       " 28.2540930518779,\n",
       " 20.69352498257757,\n",
       " 73.37594882863897,\n",
       " 25.579061683206586,\n",
       " 28.58192337963699,\n",
       " 14.98173467082145,\n",
       " 29.651323982046286,\n",
       " 26.95431593007745,\n",
       " 14.087378013390317,\n",
       " 11.22872484057493,\n",
       " 28.54497705313684,\n",
       " 38.85240759843242,\n",
       " 24.821611815681837,\n",
       " 15.14364255753828,\n",
       " 31.96356625856619,\n",
       " 11.6699307154143,\n",
       " 21.661966818636078,\n",
       " 13.56738973198894,\n",
       " 14.07807289778224,\n",
       " 14.52030213467332,\n",
       " 19.77825146183456,\n",
       " 11.050095413740468,\n",
       " 20.143095422915184,\n",
       " 24.336811182721732,\n",
       " 23.205635662165136,\n",
       " 19.650727959487725,\n",
       " 46.54523875978296,\n",
       " 13.140719589221128,\n",
       " 24.092121423115366,\n",
       " 11.280989709773142,\n",
       " 11.476282620878433,\n",
       " 27.060021508484983,\n",
       " 26.69506398882976,\n",
       " 52.080179348619076,\n",
       " 26.323192721278296,\n",
       " 74.27329870575045,\n",
       " 10.998407869158385,\n",
       " 12.062985573367591,\n",
       " 10.901325223200102,\n",
       " 14.09813834222041,\n",
       " 25.585874382785217,\n",
       " 30.745011369763393,\n",
       " 10.933137142679287,\n",
       " 68.36464601468577,\n",
       " 46.92414963551749,\n",
       " 17.036946091209195,\n",
       " 11.60658479597238,\n",
       " 13.723870621674786,\n",
       " 12.715121049896744,\n",
       " 12.631438371228679,\n",
       " 24.619543472751452,\n",
       " 15.643212010594189,\n",
       " 18.867692519376057,\n",
       " 101.13776184457235,\n",
       " 33.060160376435576,\n",
       " 69.51101281159521,\n",
       " 13.076001110169022,\n",
       " 19.62985068854058,\n",
       " 26.181569782139775,\n",
       " 11.597279147262492,\n",
       " 10.083924710446366,\n",
       " 32.19959308891726,\n",
       " 30.5343655321216,\n",
       " 20.579412054399707,\n",
       " 23.45293813085144,\n",
       " 20.73801969180746,\n",
       " 10.485957229617101,\n",
       " 18.1705440171757,\n",
       " 13.897980582094167,\n",
       " 12.072222913992551,\n",
       " 72.80149339327966,\n",
       " 28.293826484283638,\n",
       " 36.08737875791016,\n",
       " 17.577351876481494,\n",
       " 19.989378105342606,\n",
       " 15.407952756893025,\n",
       " 12.039894192763622,\n",
       " 13.945414110476706,\n",
       " 41.94169508659685,\n",
       " 32.63588368822698,\n",
       " 25.83412318337563,\n",
       " 22.890564349149425,\n",
       " 61.13259498215055,\n",
       " 11.510316591172217,\n",
       " 16.58677756320781,\n",
       " 18.61374621543159,\n",
       " 40.504307866758914,\n",
       " 107.89607031668986,\n",
       " 15.50638982756573,\n",
       " 51.47831022890391,\n",
       " 45.51253729785318,\n",
       " 15.089005595057106,\n",
       " 20.19427751304753,\n",
       " 20.541568738232673,\n",
       " 86.53587712928802,\n",
       " 13.99078430580872,\n",
       " 42.104732116383346,\n",
       " 18.1351002186979,\n",
       " 36.25124059071986,\n",
       " 20.89492297090434,\n",
       " 27.272056955616403,\n",
       " 45.748789078052866,\n",
       " 42.609713920670956,\n",
       " 50.261571248526984,\n",
       " 11.229367717278443,\n",
       " 33.55639285451645,\n",
       " 13.01616423223599,\n",
       " 20.75087346401728,\n",
       " 25.907515009003134,\n",
       " 20.308037736291077,\n",
       " 24.663209868152986,\n",
       " 123.25075007210891,\n",
       " 12.90554066679151,\n",
       " 32.50668597004573,\n",
       " 20.083653620495724,\n",
       " 12.854552087000837,\n",
       " 13.457013273473063,\n",
       " 69.62598120437667,\n",
       " 52.338963546909966,\n",
       " 57.512141498055556,\n",
       " 105.43105097269026,\n",
       " 14.671843723719196,\n",
       " 18.783726749397676,\n",
       " 32.125976889339725,\n",
       " 23.22531824026381,\n",
       " 20.839087133403467,\n",
       " 18.857460287055204,\n",
       " 28.017238997760014,\n",
       " 27.141267769584942,\n",
       " 13.521808232836884,\n",
       " 22.76332769311519,\n",
       " 26.12505338974355,\n",
       " 26.892751848263604,\n",
       " 14.717160599164801,\n",
       " 37.116963113171614,\n",
       " 10.510440573047854,\n",
       " 18.764886141589812,\n",
       " 40.301895686529086,\n",
       " 26.597349437534447,\n",
       " 16.05354365228476,\n",
       " 33.74579614228945,\n",
       " 26.75841664309887,\n",
       " 41.81660325443592,\n",
       " 36.56240999476924,\n",
       " 28.837706026463973,\n",
       " 23.395554725203247,\n",
       " 12.858527769504676,\n",
       " 16.058161854653804,\n",
       " 20.863514932567632,\n",
       " 28.290809639429813,\n",
       " 24.41891429452621,\n",
       " 16.79873133665769,\n",
       " 10.129706844667606,\n",
       " 23.713075404394708,\n",
       " 16.72697256496366,\n",
       " 36.53096867599448,\n",
       " 10.484752760185007,\n",
       " 27.581238645753366,\n",
       " 66.76576474346845,\n",
       " 13.833122180336847,\n",
       " 22.759907444939355,\n",
       " 11.917490600376132,\n",
       " 29.652986821289527,\n",
       " 25.993798143798408,\n",
       " 20.463549367425337,\n",
       " 24.73684185391598,\n",
       " 55.37810528861765,\n",
       " 14.370442489310927,\n",
       " 10.456459718600016,\n",
       " 49.561523316185415,\n",
       " 27.213198575291383,\n",
       " 54.26557719206009,\n",
       " 28.173656084406765,\n",
       " 43.23158816672084,\n",
       " 21.23092071073547,\n",
       " 37.71468486336635,\n",
       " 58.8464780974012,\n",
       " 26.978388461896127,\n",
       " 13.988177962125471,\n",
       " 16.261299942546554,\n",
       " 14.429014122145894,\n",
       " 15.670003976291037,\n",
       " 17.317642558294747,\n",
       " 26.19963372815338,\n",
       " 27.734232260072748,\n",
       " 21.119938033674174,\n",
       " 29.112109539649484,\n",
       " 11.701899030742045,\n",
       " 11.8376623930603,\n",
       " 13.77072360106581,\n",
       " 24.599439243495773,\n",
       " 14.815834329438417,\n",
       " 16.66152349325423,\n",
       " 20.22694306721135,\n",
       " 43.41202926503655,\n",
       " 10.279182251284752,\n",
       " 30.96097796540102,\n",
       " 32.10527910310044,\n",
       " 42.054786002018226,\n",
       " 63.987639299085906,\n",
       " 10.40686758809053,\n",
       " 13.339605696223046,\n",
       " 53.46121080061502,\n",
       " 22.268929846580694,\n",
       " 26.62937260979334,\n",
       " 31.75506150204313,\n",
       " 32.73043336043794,\n",
       " 63.688744881093996,\n",
       " 125.84845388072132,\n",
       " 35.47056388928972,\n",
       " 16.771683160907344,\n",
       " 52.36404695391116,\n",
       " 11.009470810409791,\n",
       " 15.255389105660967,\n",
       " 55.53556299854095,\n",
       " 15.919433783320468,\n",
       " 14.985927993408824,\n",
       " 12.972907411076845,\n",
       " 49.195766206165025,\n",
       " 23.865274114618256,\n",
       " 32.47203089288612,\n",
       " 15.46418315225472,\n",
       " 38.37969191496624,\n",
       " 38.54278919831582,\n",
       " 17.496720825838914,\n",
       " 20.575808340633543,\n",
       " 14.706607129365835,\n",
       " 85.0670994469363,\n",
       " 30.74672663774326,\n",
       " 64.52750590870811,\n",
       " 17.230463370161655,\n",
       " 40.45519884508178,\n",
       " 39.03660520096544,\n",
       " 33.530762141996306,\n",
       " 77.41664717753157,\n",
       " 20.002835863761426,\n",
       " 12.801738346040231,\n",
       " 32.27708880095066,\n",
       " 36.13560574436505,\n",
       " 27.116172920018627,\n",
       " 14.634048815041002,\n",
       " 24.04198024502916,\n",
       " 17.48484517960105,\n",
       " 21.153681583568627,\n",
       " 30.72506771473119,\n",
       " 37.346684336244266,\n",
       " 15.285163098362363,\n",
       " 20.394816952326842,\n",
       " 16.271257420216656,\n",
       " 14.428690597020424,\n",
       " 40.59287886733093,\n",
       " 57.99352994087427,\n",
       " 19.248081524660027,\n",
       " 37.82846377187728,\n",
       " 55.45961114944355,\n",
       " 47.43219127798344,\n",
       " 14.037755992063225,\n",
       " 72.60442832817702,\n",
       " 19.45379075515813,\n",
       " 18.287311838440026,\n",
       " 11.289305458331732,\n",
       " 23.317398381273954,\n",
       " 12.308522658620767,\n",
       " 15.714741195715376,\n",
       " 17.62352827628826,\n",
       " 43.09145926017286,\n",
       " 51.667168115071746,\n",
       " 17.20408952149476,\n",
       " 49.13713386326608,\n",
       " 48.930538015922224,\n",
       " 38.933710986395695,\n",
       " 13.172767349377217,\n",
       " 33.44449931590658,\n",
       " 68.0171945465159,\n",
       " 32.94809921914464,\n",
       " 18.057873813985314,\n",
       " 23.88612974315788,\n",
       " 30.75363907273512,\n",
       " 24.290293308268886,\n",
       " 12.55559795267844,\n",
       " 17.48119067077543,\n",
       " 42.429808111576065,\n",
       " 49.525868068256074,\n",
       " 12.95376621643814,\n",
       " 70.89288491029816,\n",
       " 17.9744364405191,\n",
       " 31.712760713197884,\n",
       " 37.8724344925305,\n",
       " 43.42054742139349,\n",
       " 68.28293595307372,\n",
       " 17.506077747980846,\n",
       " 49.11002356289164,\n",
       " 12.82405838070333,\n",
       " 34.418371957253605,\n",
       " 37.09418756145237,\n",
       " 32.646892064420456,\n",
       " 15.81546319580913,\n",
       " 51.2781598105108,\n",
       " 13.699103555514435,\n",
       " 11.102534579113824,\n",
       " 35.27905618072967,\n",
       " 22.905271028441664,\n",
       " 47.8224958349641,\n",
       " 19.670602538356746,\n",
       " 13.685154723787983,\n",
       " 46.21177576516078,\n",
       " 25.92175172495795,\n",
       " 14.19457349926671,\n",
       " 35.48676081195651,\n",
       " 24.312504121259625,\n",
       " 28.542634788377566,\n",
       " 22.357022279929833,\n",
       " 45.28782639932176,\n",
       " 45.387757676884725,\n",
       " 34.7153812547422,\n",
       " 12.227602553263196,\n",
       " 46.50887271324647,\n",
       " 32.23473967004878,\n",
       " 33.882044648863946,\n",
       " 19.019173712626202,\n",
       " 52.285555757949645,\n",
       " 35.846727373805884,\n",
       " 12.388973938912073,\n",
       " 20.477475449795833,\n",
       " 53.49797835132439,\n",
       " 10.96693598504864,\n",
       " 75.09867311808142,\n",
       " 32.73641455002,\n",
       " 29.959561279710364,\n",
       " 16.309542864520044,\n",
       " 15.721448405788696,\n",
       " 66.96633113701358,\n",
       " 17.084201234713696,\n",
       " 39.87806330962544,\n",
       " 19.98854328163189,\n",
       " 12.99911812241687,\n",
       " 10.215462398471827,\n",
       " 25.716539286495156,\n",
       " 84.41055524137776,\n",
       " 19.213305555735207,\n",
       " 18.838194384601998,\n",
       " 18.434683222949204,\n",
       " 24.702056235773444,\n",
       " 21.620129849427443,\n",
       " 52.39104690429634,\n",
       " 24.08500349511567,\n",
       " 24.175689695524124,\n",
       " 12.97862371249229,\n",
       " 91.18673990418645,\n",
       " 15.752918551935739,\n",
       " 11.523890556777818,\n",
       " 31.7621283208722,\n",
       " 28.09546477920762,\n",
       " 20.727848878708443,\n",
       " 20.970281003938403,\n",
       " 40.74829204503445,\n",
       " 14.355973216970655,\n",
       " 44.32413493557856,\n",
       " 30.924976833979017,\n",
       " 20.859049980151806,\n",
       " 22.250841541231594,\n",
       " 15.93169049730648,\n",
       " 37.895753161169466,\n",
       " 24.384679726237856,\n",
       " 17.738399032231566,\n",
       " 28.129289152578657,\n",
       " 119.14369261267643,\n",
       " 21.427877347937056,\n",
       " 60.688174342219334,\n",
       " 18.462915069717717,\n",
       " 11.12575083336064,\n",
       " 14.569205862621253,\n",
       " 20.849598989263114,\n",
       " 14.959868570636122,\n",
       " 10.00454127990667,\n",
       " 59.30244227605694,\n",
       " 47.14253130834183,\n",
       " 37.11858032418981,\n",
       " 13.674000424297356,\n",
       " 12.564932233318263,\n",
       " 11.399969312954665,\n",
       " 35.74920350562439,\n",
       " 62.84748548458482,\n",
       " 21.261187939093155,\n",
       " 12.53714677778658,\n",
       " 24.36490315600569,\n",
       " 18.579749268688033,\n",
       " 19.394470948646102,\n",
       " 29.421836277527298,\n",
       " 35.36795913644499,\n",
       " 12.344244946387008,\n",
       " 10.72478005822984,\n",
       " 113.45239092782803,\n",
       " 15.298287336130883,\n",
       " 11.10667153660317,\n",
       " 14.211342569258182,\n",
       " 10.794740132244357,\n",
       " 19.945388335703132,\n",
       " 42.43039048666513,\n",
       " 18.348055074936674,\n",
       " 21.746338152728043,\n",
       " 32.51474371170201,\n",
       " 24.22190862436618,\n",
       " 49.529857430761325,\n",
       " 50.20363492323845,\n",
       " 10.19920266694555,\n",
       " 40.10573013258938,\n",
       " 22.350911640267434,\n",
       " 99.90935654235571,\n",
       " 23.203227895247508,\n",
       " 96.69667168640787,\n",
       " 38.094818040727695,\n",
       " 15.716472930930626,\n",
       " 25.460913741720837,\n",
       " 106.90828105307722,\n",
       " 48.97256187171888,\n",
       " 12.048782270923942,\n",
       " 34.50883216154978,\n",
       " 18.48010597989943,\n",
       " 10.140135038694488,\n",
       " 20.332882671115648,\n",
       " 15.351169586353368,\n",
       " 44.17072492208756,\n",
       " 18.22958138084858,\n",
       " 45.149908163979376,\n",
       " 10.011962353430258,\n",
       " 17.358160272318848,\n",
       " 15.519423978812442,\n",
       " 15.320169318002256,\n",
       " 11.412454402925256,\n",
       " 14.143540705023371,\n",
       " 26.218334970993258,\n",
       " 16.888183054948225,\n",
       " 20.804909573190713,\n",
       " 19.459902156296486,\n",
       " 51.734145465910686,\n",
       " 57.14061417470509,\n",
       " 23.583188209264165,\n",
       " 18.01420710710814,\n",
       " 12.258910943928619,\n",
       " 71.32433058149837,\n",
       " 11.242337272243576,\n",
       " 17.06647637809341,\n",
       " 17.078323661283406,\n",
       " 17.725055633641055,\n",
       " 13.589396774734222,\n",
       " 40.57204247191606,\n",
       " 12.613517742781784,\n",
       " 30.152044594901334,\n",
       " 12.644268697811425,\n",
       " 11.462936263226268,\n",
       " 58.03253533429852,\n",
       " 14.21760029433356,\n",
       " 44.435359241561365,\n",
       " 25.732938332853372,\n",
       " 11.62108897774565,\n",
       " 72.22205059323764,\n",
       " 15.672769629041625,\n",
       " 49.27165624308441,\n",
       " 13.697937540554232,\n",
       " 48.47515031146406,\n",
       " 10.266341861527488,\n",
       " 24.339801804351154,\n",
       " 39.345106688607544,\n",
       " 64.6045412460405,\n",
       " 57.98239040309607,\n",
       " 44.254330773195676,\n",
       " 53.8193269584141,\n",
       " 14.443372679938362,\n",
       " 16.686650827980436,\n",
       " 16.820771628239502,\n",
       " 19.46590661115357,\n",
       " 19.995098296115927,\n",
       " 25.72962057669807,\n",
       " 13.484205170773818,\n",
       " 33.50425171093756,\n",
       " 13.668039748287574,\n",
       " 17.430217137454463,\n",
       " 23.997198437822863,\n",
       " 41.835944715622304,\n",
       " 36.900686363871245,\n",
       " 30.74287705655778,\n",
       " 13.386683437281196,\n",
       " 25.839265184123036,\n",
       " 11.830171341590901,\n",
       " 19.475837569686867,\n",
       " 20.672215918468254,\n",
       " 10.8125119312718,\n",
       " 16.403777447540953,\n",
       " 72.3126111653828,\n",
       " 71.42317643496284,\n",
       " 18.68738846786406,\n",
       " 10.881130001048446,\n",
       " 13.946956297242957,\n",
       " 19.95877348147286,\n",
       " 69.0456786382764,\n",
       " 13.044362450926457,\n",
       " 38.83748131097656,\n",
       " 12.301092359438584,\n",
       " 89.53931471564043,\n",
       " 19.612929247672664,\n",
       " 39.853956311125174,\n",
       " 18.388981070331177,\n",
       " 29.759434109241532,\n",
       " 14.264262550313337,\n",
       " 13.15697396512751,\n",
       " 65.63311877148783,\n",
       " 59.93487535413324,\n",
       " 53.69052016892125,\n",
       " 29.639715773044703,\n",
       " 43.03315633987162,\n",
       " 11.430713538808599,\n",
       " 18.153021167871522,\n",
       " 17.52889964252741,\n",
       " 30.192549910306884,\n",
       " 18.79981971764755,\n",
       " 13.21527486521411,\n",
       " 28.149943942258158,\n",
       " 43.681496561130075,\n",
       " 19.629333414324467,\n",
       " 12.837463837159826,\n",
       " 14.380246863178717,\n",
       " 15.422617517608701,\n",
       " 17.130573178336338,\n",
       " 30.588912594014857,\n",
       " 28.37314482388106,\n",
       " 33.81980629222845,\n",
       " 19.0040822989002,\n",
       " 44.745158848839026,\n",
       " 14.56532452564574,\n",
       " 22.66139996013962,\n",
       " 42.28285139412692,\n",
       " 13.972888605735621,\n",
       " 12.327600465941156,\n",
       " 53.02888751777288,\n",
       " 18.39928416045332,\n",
       " 33.03204305416183,\n",
       " 10.513508295055555,\n",
       " 33.42668491753064,\n",
       " 17.598909772532583,\n",
       " 11.147092620107848,\n",
       " 28.8225711294866,\n",
       " 15.509901162779062,\n",
       " 19.781518846299008,\n",
       " 11.680262520134338,\n",
       " 13.183742688866944,\n",
       " 17.075788948270116,\n",
       " 11.880928585793354,\n",
       " 58.343374598204974,\n",
       " 10.121308011847747,\n",
       " 23.227553130236508,\n",
       " 12.63060981546137,\n",
       " 17.354113746625785,\n",
       " 62.08990744525223,\n",
       " 13.52745500024811,\n",
       " 77.5295020546265,\n",
       " 14.608257604322922,\n",
       " 37.062307129804154,\n",
       " 25.823019352867632,\n",
       " 53.1660117865458,\n",
       " 26.148424097722454,\n",
       " 25.295408131807164,\n",
       " 14.349642160619231,\n",
       " 13.635938577944584,\n",
       " 59.655574576095376,\n",
       " 17.94512383839957,\n",
       " 19.306922006654965,\n",
       " 38.52763737786412,\n",
       " 18.274688570439316,\n",
       " 70.75712173436884,\n",
       " 36.78270244745008,\n",
       " 28.197688054499533,\n",
       " 52.55502339802167,\n",
       " 11.017570941815368,\n",
       " 15.942872298591737,\n",
       " 22.511128976480546,\n",
       " 15.148969362601058,\n",
       " 37.24415081937438,\n",
       " 60.31755425248586,\n",
       " 21.024972011245207,\n",
       " 18.375037757831613,\n",
       " 75.16831249001763,\n",
       " 21.770426275602176,\n",
       " 23.42960820772535,\n",
       " 18.666488161807585,\n",
       " 58.05882370324289,\n",
       " 26.83087814326118,\n",
       " 38.72436868167779,\n",
       " 43.40571500800981,\n",
       " 18.36641908781664,\n",
       " 21.37242591766259,\n",
       " 47.73966727672802,\n",
       " 19.027731396626518,\n",
       " 41.5912854720547,\n",
       " 66.60175435370108,\n",
       " 29.521365320651217,\n",
       " 37.06704228487857,\n",
       " 42.705097621734694,\n",
       " 47.29171988236595,\n",
       " 20.052299731690823,\n",
       " 11.052482731853672,\n",
       " 20.279152842937126,\n",
       " 32.4022083130239,\n",
       " 21.969485376475518,\n",
       " 37.64311082609331,\n",
       " 18.831664585052856,\n",
       " 30.143143753940716,\n",
       " 14.12621553586591,\n",
       " 37.6755056648463,\n",
       " 25.77977265706334,\n",
       " 14.564739730869153,\n",
       " 16.903714639934428,\n",
       " 37.61489247534796,\n",
       " 23.178827948925626,\n",
       " 14.867727239560832,\n",
       " 78.38999121050817,\n",
       " 35.114097219594825,\n",
       " 72.99391460341037,\n",
       " 21.141058086194473,\n",
       " 22.89096319568669,\n",
       " 55.646870100911514,\n",
       " 24.89088709087208,\n",
       " 34.57070082186601,\n",
       " 15.811668947327032,\n",
       " 21.838055362178274,\n",
       " 12.635227561877134,\n",
       " 14.612933486386268,\n",
       " 10.018877211278058,\n",
       " 10.801134324421849,\n",
       " 85.69355945842469,\n",
       " 15.55077237028858,\n",
       " 31.771895211593257,\n",
       " 46.53940706025939,\n",
       " 25.99952923035993,\n",
       " 13.324504022171338,\n",
       " 36.04762765559322,\n",
       " 28.33838909455822,\n",
       " 36.27951784926269,\n",
       " 39.37149873333865,\n",
       " 84.10860750956958,\n",
       " 27.04582587156467,\n",
       " 18.36010149804356,\n",
       " 30.908894779211515,\n",
       " 11.417908802524302,\n",
       " 55.53782418741062,\n",
       " 12.542604451409154,\n",
       " 17.94526187218206,\n",
       " 43.82873353996634,\n",
       " 28.191882998802512]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expon(10,20).rvs(1000).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
       "       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n",
       "       69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n",
       "       86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(1,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 0.9295 - val_loss: 0.5767\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4874 - val_loss: 0.5132\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4426 - val_loss: 0.4762\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4110 - val_loss: 0.4519\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3892 - val_loss: 0.4409\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3752 - val_loss: 0.4304\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3655 - val_loss: 0.4286\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3573 - val_loss: 0.4217\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3527 - val_loss: 0.4309\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3477 - val_loss: 0.4167\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3438 - val_loss: 0.4247\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3387 - val_loss: 0.4199\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3337 - val_loss: 0.4200\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3333 - val_loss: 0.4137\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3285 - val_loss: 0.4136\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3284 - val_loss: 0.4089\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3237 - val_loss: 0.4097\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3231 - val_loss: 0.4121\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3178 - val_loss: 0.4026\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3167 - val_loss: 0.4073\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3140 - val_loss: 0.4022\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3127 - val_loss: 0.4039\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3103 - val_loss: 0.4095\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3116 - val_loss: 0.4273\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3078 - val_loss: 0.4130\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3040 - val_loss: 0.3981\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3039 - val_loss: 0.4138\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3023 - val_loss: 0.4052\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2998 - val_loss: 0.3968\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2981 - val_loss: 0.4012\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2974 - val_loss: 0.3998\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2964 - val_loss: 0.3968\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2938 - val_loss: 0.3913\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2916 - val_loss: 0.4151\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2911 - val_loss: 0.3921\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2892 - val_loss: 0.3936\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2896 - val_loss: 0.3951\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2879 - val_loss: 0.3968\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2863 - val_loss: 0.4017\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2863 - val_loss: 0.3877\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2842 - val_loss: 0.3887\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2826 - val_loss: 0.3968\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2827 - val_loss: 0.3963\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2815 - val_loss: 0.4074\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2818 - val_loss: 0.3958\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2810 - val_loss: 0.3912\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2783 - val_loss: 0.3811\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2786 - val_loss: 0.3846\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2764 - val_loss: 0.3938\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2774 - val_loss: 0.3982\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2768 - val_loss: 0.4155\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2738 - val_loss: 0.3958\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2758 - val_loss: 0.3949\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2731 - val_loss: 0.4242\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2714 - val_loss: 0.3938\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2731 - val_loss: 0.4031\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2709 - val_loss: 0.4038\n",
      "121/121 [==============================] - 0s 675us/step - loss: 0.3060\n",
      "[CV] END learning_rate=0.008744254635814247, n_hidden=3, n_neurons=31; total time=  15.0s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 0.8702 - val_loss: 0.5584\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5029 - val_loss: 0.5044\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4311 - val_loss: 0.4590\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3994 - val_loss: 0.4329\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3777 - val_loss: 0.4283\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3653 - val_loss: 0.4279\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3573 - val_loss: 0.4171\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3524 - val_loss: 0.4169\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3453 - val_loss: 0.4265\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3414 - val_loss: 0.4133\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3365 - val_loss: 0.4156\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3327 - val_loss: 0.4087\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3303 - val_loss: 0.4144\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3298 - val_loss: 0.4062\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3263 - val_loss: 0.4020\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3219 - val_loss: 0.3995\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3219 - val_loss: 0.3982\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3198 - val_loss: 0.4190\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3170 - val_loss: 0.3970\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3143 - val_loss: 0.3957\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3135 - val_loss: 0.3971\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3120 - val_loss: 0.3914\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3093 - val_loss: 0.4244\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3093 - val_loss: 0.3985\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3064 - val_loss: 0.4152\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3052 - val_loss: 0.3941\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3031 - val_loss: 0.4022\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3026 - val_loss: 0.3903\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3016 - val_loss: 0.3935\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3024 - val_loss: 0.4069\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2973 - val_loss: 0.3905\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2954 - val_loss: 0.4009\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2956 - val_loss: 0.3941\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2931 - val_loss: 0.3994\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2926 - val_loss: 0.3931\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2922 - val_loss: 0.4017\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2904 - val_loss: 0.3854\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2895 - val_loss: 0.4106\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2900 - val_loss: 0.3981\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2880 - val_loss: 0.3941\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2858 - val_loss: 0.4212\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2852 - val_loss: 0.3970\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2846 - val_loss: 0.3973\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2842 - val_loss: 0.4092\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2834 - val_loss: 0.4084\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2837 - val_loss: 0.3917\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2829 - val_loss: 0.3871\n",
      "121/121 [==============================] - 0s 683us/step - loss: 0.3125\n",
      "[CV] END learning_rate=0.008744254635814247, n_hidden=3, n_neurons=31; total time=  12.6s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 1.1141 - val_loss: 0.9331\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6485 - val_loss: 0.5496\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4661 - val_loss: 0.4773\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4106 - val_loss: 0.4518\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3865 - val_loss: 0.4457\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3759 - val_loss: 0.4409\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3693 - val_loss: 0.4246\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3639 - val_loss: 0.4300\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3574 - val_loss: 0.4316\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3543 - val_loss: 0.4201\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3490 - val_loss: 0.4179\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3484 - val_loss: 0.4226\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3430 - val_loss: 0.4188\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3402 - val_loss: 0.4148\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3392 - val_loss: 0.4233\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3370 - val_loss: 0.4211\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3362 - val_loss: 0.4130\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3302 - val_loss: 0.4194\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3285 - val_loss: 0.4126\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3251 - val_loss: 0.4133\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3230 - val_loss: 0.4088\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3220 - val_loss: 0.4124\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3203 - val_loss: 0.4132\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3185 - val_loss: 0.4107\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3153 - val_loss: 0.4115\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3133 - val_loss: 0.4027\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3142 - val_loss: 0.4190\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3188 - val_loss: 0.4079\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.3158 - val_loss: 0.4058\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3136 - val_loss: 0.4117\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3075 - val_loss: 0.4015\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3059 - val_loss: 0.4016\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3055 - val_loss: 0.4046\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3026 - val_loss: 0.3977\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 995us/step - loss: 0.3014 - val_loss: 0.4172\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2998 - val_loss: 0.4049\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2992 - val_loss: 0.3977\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2987 - val_loss: 0.4053\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2985 - val_loss: 0.3953\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2974 - val_loss: 0.4000\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2994 - val_loss: 0.4152\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2959 - val_loss: 0.3993\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2944 - val_loss: 0.3993\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 998us/step - loss: 0.2920 - val_loss: 0.4045\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2932 - val_loss: 0.4162\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2904 - val_loss: 0.4196\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2897 - val_loss: 0.4001\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2912 - val_loss: 0.3943\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2882 - val_loss: 0.4069\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2880 - val_loss: 0.3911\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 999us/step - loss: 0.2864 - val_loss: 0.4038\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2863 - val_loss: 0.3954\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2865 - val_loss: 0.4031\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2863 - val_loss: 0.4046\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.2838 - val_loss: 0.3935\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2826 - val_loss: 0.3964\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2831 - val_loss: 0.4049\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2854 - val_loss: 0.3976\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2830 - val_loss: 0.3885\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.2830 - val_loss: 0.4021\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2815 - val_loss: 0.4063\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2813 - val_loss: 0.3945\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2797 - val_loss: 0.4088\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.2813 - val_loss: 0.4063\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2803 - val_loss: 0.3983\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2781 - val_loss: 0.4032\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.2781 - val_loss: 0.3931\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 997us/step - loss: 0.2781 - val_loss: 0.4018\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2777 - val_loss: 0.4038\n",
      "121/121 [==============================] - 0s 625us/step - loss: 0.2940\n",
      "[CV] END learning_rate=0.008744254635814247, n_hidden=3, n_neurons=31; total time=  17.7s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 0.6333 - val_loss: 0.4820\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4345 - val_loss: 0.4881\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4252 - val_loss: 0.4715\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6683 - val_loss: 0.4621\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4852 - val_loss: 0.7235\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6918 - val_loss: 0.4255\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3675 - val_loss: 0.4343\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3656 - val_loss: 0.4264\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3386 - val_loss: 0.4405\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3300 - val_loss: 0.4209\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3268 - val_loss: 0.4314\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3202 - val_loss: 0.4097\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3123 - val_loss: 0.4230\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3100 - val_loss: 0.4040\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3046 - val_loss: 0.4003\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3041 - val_loss: 0.3953\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3005 - val_loss: 0.4012\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2991 - val_loss: 0.4090\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2936 - val_loss: 0.3944\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2913 - val_loss: 0.3955\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2890 - val_loss: 0.3933\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2860 - val_loss: 0.4084\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2838 - val_loss: 0.3937\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2853 - val_loss: 0.4083\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2831 - val_loss: 0.4108\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2766 - val_loss: 0.3828\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2808 - val_loss: 0.4106\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2792 - val_loss: 0.3926\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2762 - val_loss: 0.3883\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2700 - val_loss: 0.4245\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2705 - val_loss: 0.3958\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2709 - val_loss: 0.3976\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2673 - val_loss: 0.3805\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2651 - val_loss: 0.4044\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2618 - val_loss: 0.4078\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2596 - val_loss: 0.3932\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2610 - val_loss: 0.3924\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2590 - val_loss: 0.4104\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2588 - val_loss: 0.3955\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2589 - val_loss: 0.3941\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2568 - val_loss: 0.3882\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2533 - val_loss: 0.4258\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2554 - val_loss: 0.4014\n",
      "121/121 [==============================] - 0s 642us/step - loss: 0.2897\n",
      "[CV] END learning_rate=0.022609577469652482, n_hidden=3, n_neurons=57; total time=  11.2s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 0.6054 - val_loss: 0.4517\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4055 - val_loss: 0.4547\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3736 - val_loss: 0.4221\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3615 - val_loss: 0.4075\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3516 - val_loss: 0.4211\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3436 - val_loss: 0.4082\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3381 - val_loss: 0.3948\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3338 - val_loss: 0.4054\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3264 - val_loss: 0.4161\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3224 - val_loss: 0.3978\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3157 - val_loss: 0.4065\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3110 - val_loss: 0.3952\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3087 - val_loss: 0.3945\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3079 - val_loss: 0.3867\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3050 - val_loss: 0.3968\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2999 - val_loss: 0.3890\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2978 - val_loss: 0.3829\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2968 - val_loss: 0.3953\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2919 - val_loss: 0.4617\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2899 - val_loss: 0.3906\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2890 - val_loss: 0.3802\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2878 - val_loss: 0.4004\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2816 - val_loss: 0.4052\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2863 - val_loss: 0.4316\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2809 - val_loss: 0.3943\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2772 - val_loss: 0.3738\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2744 - val_loss: 0.3918\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2737 - val_loss: 0.3912\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2724 - val_loss: 0.3954\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2763 - val_loss: 0.4335\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2696 - val_loss: 0.4125\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2709 - val_loss: 0.4012\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2704 - val_loss: 0.4251\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2651 - val_loss: 0.4158\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2645 - val_loss: 0.3956\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2628 - val_loss: 0.4041\n",
      "121/121 [==============================] - 0s 642us/step - loss: 0.3192\n",
      "[CV] END learning_rate=0.022609577469652482, n_hidden=3, n_neurons=57; total time=   9.4s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 0.7015 - val_loss: 0.5234\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4253 - val_loss: 0.4369\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3951 - val_loss: 0.4549\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4247 - val_loss: 0.4295\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3857 - val_loss: 0.4427\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3647 - val_loss: 0.4499\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3879 - val_loss: 0.4379\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3558 - val_loss: 0.4247\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3527 - val_loss: 0.4241\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3414 - val_loss: 0.4110\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3286 - val_loss: 0.3911\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3264 - val_loss: 0.4219\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3193 - val_loss: 0.3966\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3149 - val_loss: 0.4170\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3108 - val_loss: 0.4035\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3063 - val_loss: 0.4198\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3072 - val_loss: 0.3840\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3025 - val_loss: 0.4068\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3002 - val_loss: 0.4010\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2991 - val_loss: 0.4011\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2946 - val_loss: 0.4026\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2945 - val_loss: 0.4046\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2907 - val_loss: 0.4068\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2904 - val_loss: 0.3999\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2892 - val_loss: 0.4212\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2845 - val_loss: 0.4083\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2818 - val_loss: 0.3958\n",
      "121/121 [==============================] - 0s 617us/step - loss: 0.2868\n",
      "[CV] END learning_rate=0.022609577469652482, n_hidden=3, n_neurons=57; total time=   7.2s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 3.5528 - val_loss: 1.9659\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.5610 - val_loss: 1.2086\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0910 - val_loss: 0.9460\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8913 - val_loss: 0.7986\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7833 - val_loss: 0.7164\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7244 - val_loss: 0.6713\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6898 - val_loss: 0.6435\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6661 - val_loss: 0.6248\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6460 - val_loss: 0.6098\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6269 - val_loss: 0.5949\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6097 - val_loss: 0.5832\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5935 - val_loss: 0.5722\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5775 - val_loss: 0.5593\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5631 - val_loss: 0.5510\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5491 - val_loss: 0.5390\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5355 - val_loss: 0.5304\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5232 - val_loss: 0.5213\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5111 - val_loss: 0.5134\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4997 - val_loss: 0.5080\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4894 - val_loss: 0.4977\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4797 - val_loss: 0.4940\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4701 - val_loss: 0.4869\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4616 - val_loss: 0.4810\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4533 - val_loss: 0.4743\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4462 - val_loss: 0.4705\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4390 - val_loss: 0.4667\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4327 - val_loss: 0.4622\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4265 - val_loss: 0.4596\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4206 - val_loss: 0.4549\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4156 - val_loss: 0.4525\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4106 - val_loss: 0.4491\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4062 - val_loss: 0.4480\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4020 - val_loss: 0.4447\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3980 - val_loss: 0.4417\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3945 - val_loss: 0.4399\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3912 - val_loss: 0.4379\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3882 - val_loss: 0.4355\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3854 - val_loss: 0.4358\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3828 - val_loss: 0.4317\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3804 - val_loss: 0.4325\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3779 - val_loss: 0.4313\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3757 - val_loss: 0.4307\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3739 - val_loss: 0.4283\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3717 - val_loss: 0.4280\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3702 - val_loss: 0.4273\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3681 - val_loss: 0.4255\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3668 - val_loss: 0.4248\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3652 - val_loss: 0.4238\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3637 - val_loss: 0.4228\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3623 - val_loss: 0.4225\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3609 - val_loss: 0.4224\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3600 - val_loss: 0.4208\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3587 - val_loss: 0.4206\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3575 - val_loss: 0.4197\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3560 - val_loss: 0.4198\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3555 - val_loss: 0.4184\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3543 - val_loss: 0.4186\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3532 - val_loss: 0.4167\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3523 - val_loss: 0.4171\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3513 - val_loss: 0.4166\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3507 - val_loss: 0.4163\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3497 - val_loss: 0.4170\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3490 - val_loss: 0.4160\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3483 - val_loss: 0.4156\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3474 - val_loss: 0.4153\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3470 - val_loss: 0.4156\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3461 - val_loss: 0.4152\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3455 - val_loss: 0.4152\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3447 - val_loss: 0.4137\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3441 - val_loss: 0.4131\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3435 - val_loss: 0.4138\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3429 - val_loss: 0.4124\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3422 - val_loss: 0.4130\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3417 - val_loss: 0.4122\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3412 - val_loss: 0.4128\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3404 - val_loss: 0.4123\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3403 - val_loss: 0.4127\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3394 - val_loss: 0.4132\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3390 - val_loss: 0.4119\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3384 - val_loss: 0.4128\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3379 - val_loss: 0.4136\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3375 - val_loss: 0.4111\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3371 - val_loss: 0.4117\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3365 - val_loss: 0.4112\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3361 - val_loss: 0.4124\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3353 - val_loss: 0.4108\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3353 - val_loss: 0.4104\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3347 - val_loss: 0.4117\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3345 - val_loss: 0.4105\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3339 - val_loss: 0.4104\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3336 - val_loss: 0.4105\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3331 - val_loss: 0.4092\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3327 - val_loss: 0.4103\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3324 - val_loss: 0.4092\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3320 - val_loss: 0.4089\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3312 - val_loss: 0.4094\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3311 - val_loss: 0.4084\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3308 - val_loss: 0.4101\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3300 - val_loss: 0.4101\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3302 - val_loss: 0.4084\n",
      "121/121 [==============================] - 0s 625us/step - loss: 0.3537\n",
      "[CV] END learning_rate=0.0005504805791032996, n_hidden=3, n_neurons=64; total time=  25.4s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 3.0159 - val_loss: 1.3136\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9911 - val_loss: 0.8702\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7731 - val_loss: 0.7570\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7010 - val_loss: 0.6976\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6617 - val_loss: 0.6596\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6347 - val_loss: 0.6340\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6128 - val_loss: 0.6135\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5938 - val_loss: 0.5947\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5766 - val_loss: 0.5805\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5609 - val_loss: 0.5682\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5463 - val_loss: 0.5570\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5329 - val_loss: 0.5458\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5206 - val_loss: 0.5369\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5091 - val_loss: 0.5307\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4983 - val_loss: 0.5215\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4884 - val_loss: 0.5167\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4791 - val_loss: 0.5088\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4705 - val_loss: 0.5035\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4625 - val_loss: 0.4979\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4553 - val_loss: 0.4945\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4487 - val_loss: 0.4887\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4425 - val_loss: 0.4856\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4369 - val_loss: 0.4814\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4319 - val_loss: 0.4770\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4273 - val_loss: 0.4734\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4230 - val_loss: 0.4706\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4190 - val_loss: 0.4685\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4155 - val_loss: 0.4654\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4119 - val_loss: 0.4656\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4094 - val_loss: 0.4612\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4062 - val_loss: 0.4573\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4036 - val_loss: 0.4564\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4013 - val_loss: 0.4548\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3989 - val_loss: 0.4543\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3968 - val_loss: 0.4524\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3948 - val_loss: 0.4489\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3929 - val_loss: 0.4489\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3911 - val_loss: 0.4476\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3893 - val_loss: 0.4444\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3878 - val_loss: 0.4446\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3860 - val_loss: 0.4436\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3846 - val_loss: 0.4438\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3830 - val_loss: 0.4403\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3818 - val_loss: 0.4401\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3803 - val_loss: 0.4411\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3790 - val_loss: 0.4368\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3777 - val_loss: 0.4356\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3765 - val_loss: 0.4352\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3751 - val_loss: 0.4358\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3742 - val_loss: 0.4343\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3727 - val_loss: 0.4316\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3720 - val_loss: 0.4314\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3707 - val_loss: 0.4302\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3696 - val_loss: 0.4307\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3687 - val_loss: 0.4306\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3677 - val_loss: 0.4292\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3666 - val_loss: 0.4269\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3659 - val_loss: 0.4280\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3649 - val_loss: 0.4274\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3638 - val_loss: 0.4262\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3632 - val_loss: 0.4264\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3623 - val_loss: 0.4277\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3615 - val_loss: 0.4245\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3608 - val_loss: 0.4249\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3597 - val_loss: 0.4266\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3591 - val_loss: 0.4266\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3585 - val_loss: 0.4237\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3575 - val_loss: 0.4246\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3569 - val_loss: 0.4236\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3561 - val_loss: 0.4204\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3555 - val_loss: 0.4209\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3548 - val_loss: 0.4213\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3541 - val_loss: 0.4202\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3536 - val_loss: 0.4208\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3530 - val_loss: 0.4207\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3522 - val_loss: 0.4209\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3516 - val_loss: 0.4194\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3511 - val_loss: 0.4198\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3506 - val_loss: 0.4196\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3499 - val_loss: 0.4189\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3493 - val_loss: 0.4192\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3488 - val_loss: 0.4181\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3480 - val_loss: 0.4206\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3477 - val_loss: 0.4181\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3471 - val_loss: 0.4175\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3465 - val_loss: 0.4167\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3461 - val_loss: 0.4169\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3455 - val_loss: 0.4177\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3451 - val_loss: 0.4177\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3445 - val_loss: 0.4171\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3440 - val_loss: 0.4155\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3435 - val_loss: 0.4168\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3430 - val_loss: 0.4155\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3425 - val_loss: 0.4152\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3418 - val_loss: 0.4164\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3415 - val_loss: 0.4155\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3409 - val_loss: 0.4133\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3406 - val_loss: 0.4134\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3401 - val_loss: 0.4149\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3397 - val_loss: 0.4138\n",
      "121/121 [==============================] - 0s 632us/step - loss: 0.3599\n",
      "[CV] END learning_rate=0.0005504805791032996, n_hidden=3, n_neurons=64; total time=  25.7s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 3.1626 - val_loss: 1.4202\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1111 - val_loss: 0.8398\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7782 - val_loss: 0.7457\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7128 - val_loss: 0.7092\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6811 - val_loss: 0.6841\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6573 - val_loss: 0.6637\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6355 - val_loss: 0.6477\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6175 - val_loss: 0.6305\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6008 - val_loss: 0.6172\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5852 - val_loss: 0.6053\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5709 - val_loss: 0.5945\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5578 - val_loss: 0.5831\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5453 - val_loss: 0.5735\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5341 - val_loss: 0.5669\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5234 - val_loss: 0.5585\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5134 - val_loss: 0.5490\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5039 - val_loss: 0.5422\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4950 - val_loss: 0.5344\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4870 - val_loss: 0.5288\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4791 - val_loss: 0.5232\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4718 - val_loss: 0.5177\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4650 - val_loss: 0.5134\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4586 - val_loss: 0.5076\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4521 - val_loss: 0.5053\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4467 - val_loss: 0.4992\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4412 - val_loss: 0.4944\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4361 - val_loss: 0.4902\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4314 - val_loss: 0.4871\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4268 - val_loss: 0.4856\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4230 - val_loss: 0.4794\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4190 - val_loss: 0.4763\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4152 - val_loss: 0.4751\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4117 - val_loss: 0.4702\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4085 - val_loss: 0.4702\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4059 - val_loss: 0.4666\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4026 - val_loss: 0.4635\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4002 - val_loss: 0.4610\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3973 - val_loss: 0.4599\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3950 - val_loss: 0.4576\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3926 - val_loss: 0.4555\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3902 - val_loss: 0.4553\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3885 - val_loss: 0.4531\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3863 - val_loss: 0.4501\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3847 - val_loss: 0.4491\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3829 - val_loss: 0.4488\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3812 - val_loss: 0.4454\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3795 - val_loss: 0.4441\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3782 - val_loss: 0.4427\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3762 - val_loss: 0.4419\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3754 - val_loss: 0.4408\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3738 - val_loss: 0.4402\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3725 - val_loss: 0.4389\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3714 - val_loss: 0.4372\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3701 - val_loss: 0.4368\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3686 - val_loss: 0.4361\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3674 - val_loss: 0.4352\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3663 - val_loss: 0.4341\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3655 - val_loss: 0.4332\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3643 - val_loss: 0.4322\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3631 - val_loss: 0.4317\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3623 - val_loss: 0.4315\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3610 - val_loss: 0.4318\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3607 - val_loss: 0.4294\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3595 - val_loss: 0.4296\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3586 - val_loss: 0.4286\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3575 - val_loss: 0.4282\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3569 - val_loss: 0.4270\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3560 - val_loss: 0.4273\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3552 - val_loss: 0.4268\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3543 - val_loss: 0.4266\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3535 - val_loss: 0.4242\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3526 - val_loss: 0.4256\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3518 - val_loss: 0.4259\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3512 - val_loss: 0.4240\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3503 - val_loss: 0.4239\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3492 - val_loss: 0.4254\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3490 - val_loss: 0.4237\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3481 - val_loss: 0.4234\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3474 - val_loss: 0.4235\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3467 - val_loss: 0.4238\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3463 - val_loss: 0.4217\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3456 - val_loss: 0.4229\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3450 - val_loss: 0.4224\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3443 - val_loss: 0.4215\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3438 - val_loss: 0.4222\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3435 - val_loss: 0.4205\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3426 - val_loss: 0.4210\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3424 - val_loss: 0.4206\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3416 - val_loss: 0.4212\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3412 - val_loss: 0.4200\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3405 - val_loss: 0.4212\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3403 - val_loss: 0.4188\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3395 - val_loss: 0.4192\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3394 - val_loss: 0.4186\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3385 - val_loss: 0.4200\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3383 - val_loss: 0.4192\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3376 - val_loss: 0.4175\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3373 - val_loss: 0.4178\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3369 - val_loss: 0.4178\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3366 - val_loss: 0.4184\n",
      "121/121 [==============================] - 0s 667us/step - loss: 0.3408\n",
      "[CV] END learning_rate=0.0005504805791032996, n_hidden=3, n_neurons=64; total time=  25.8s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.1689 - val_loss: 5.0184\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 4.2188 - val_loss: 3.4607\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 919us/step - loss: 2.9708 - val_loss: 2.4597\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 2.1638 - val_loss: 1.8106\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 910us/step - loss: 1.6370 - val_loss: 1.3857\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 904us/step - loss: 1.2905 - val_loss: 1.1068\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 910us/step - loss: 1.0619 - val_loss: 0.9233\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 0.9103 - val_loss: 0.8017\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 903us/step - loss: 0.8092 - val_loss: 0.7210\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 918us/step - loss: 0.7410 - val_loss: 0.6669\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 916us/step - loss: 0.6950 - val_loss: 0.6306\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 918us/step - loss: 0.6638 - val_loss: 0.6060\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.6419 - val_loss: 0.5891\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 931us/step - loss: 0.6267 - val_loss: 0.5773\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 893us/step - loss: 0.6160 - val_loss: 0.5689\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 0.6080 - val_loss: 0.5629\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.6021 - val_loss: 0.5585\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 910us/step - loss: 0.5974 - val_loss: 0.5550\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 0.5937 - val_loss: 0.5521\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 918us/step - loss: 0.5907 - val_loss: 0.5498\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 920us/step - loss: 0.5883 - val_loss: 0.5479\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 904us/step - loss: 0.5857 - val_loss: 0.5460\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 906us/step - loss: 0.5837 - val_loss: 0.5445\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 907us/step - loss: 0.5818 - val_loss: 0.5431\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 903us/step - loss: 0.5801 - val_loss: 0.5417\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5784 - val_loss: 0.5403\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 0.5765 - val_loss: 0.5391\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 914us/step - loss: 0.5752 - val_loss: 0.5378\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 0.5736 - val_loss: 0.5366\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 910us/step - loss: 0.5722 - val_loss: 0.5354\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.5708 - val_loss: 0.5343\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.5695 - val_loss: 0.5332\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 904us/step - loss: 0.5681 - val_loss: 0.5321\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 0.5669 - val_loss: 0.5311\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 0.5658 - val_loss: 0.5301\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 918us/step - loss: 0.5644 - val_loss: 0.5291\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 906us/step - loss: 0.5635 - val_loss: 0.5284\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 933us/step - loss: 0.5621 - val_loss: 0.5274\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.5612 - val_loss: 0.5265\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.5601 - val_loss: 0.5257\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 916us/step - loss: 0.5590 - val_loss: 0.5250\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 906us/step - loss: 0.5582 - val_loss: 0.5242\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 911us/step - loss: 0.5572 - val_loss: 0.5235\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 911us/step - loss: 0.5561 - val_loss: 0.5227\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 911us/step - loss: 0.5553 - val_loss: 0.5220\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 904us/step - loss: 0.5545 - val_loss: 0.5213\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 907us/step - loss: 0.5535 - val_loss: 0.5206\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 0.5528 - val_loss: 0.5200\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 898us/step - loss: 0.5520 - val_loss: 0.5194\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 907us/step - loss: 0.5510 - val_loss: 0.5188\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 911us/step - loss: 0.5505 - val_loss: 0.5183\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.5498 - val_loss: 0.5177\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 916us/step - loss: 0.5490 - val_loss: 0.5172\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 904us/step - loss: 0.5484 - val_loss: 0.5168\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.5475 - val_loss: 0.5162\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 928us/step - loss: 0.5471 - val_loss: 0.5158\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 911us/step - loss: 0.5463 - val_loss: 0.5154\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 914us/step - loss: 0.5457 - val_loss: 0.5149\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.5452 - val_loss: 0.5145\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 929us/step - loss: 0.5446 - val_loss: 0.5142\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 907us/step - loss: 0.5440 - val_loss: 0.5137\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 918us/step - loss: 0.5434 - val_loss: 0.5134\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 906us/step - loss: 0.5429 - val_loss: 0.5129\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.5424 - val_loss: 0.5125\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 906us/step - loss: 0.5418 - val_loss: 0.5122\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.5414 - val_loss: 0.5119\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 897us/step - loss: 0.5409 - val_loss: 0.5116\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 907us/step - loss: 0.5406 - val_loss: 0.5113\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 0.5399 - val_loss: 0.5110\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 919us/step - loss: 0.5396 - val_loss: 0.5107\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.5390 - val_loss: 0.5103\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.5388 - val_loss: 0.5100\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.5384 - val_loss: 0.5097\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 924us/step - loss: 0.5379 - val_loss: 0.5095\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 918us/step - loss: 0.5376 - val_loss: 0.5093\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.5371 - val_loss: 0.5092\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 939us/step - loss: 0.5370 - val_loss: 0.5090\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 927us/step - loss: 0.5364 - val_loss: 0.5088\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 907us/step - loss: 0.5361 - val_loss: 0.5085\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 956us/step - loss: 0.5359 - val_loss: 0.5082\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 894us/step - loss: 0.5354 - val_loss: 0.5081\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.5353 - val_loss: 0.5080\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 923us/step - loss: 0.5350 - val_loss: 0.5078\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 907us/step - loss: 0.5347 - val_loss: 0.5076\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.5343 - val_loss: 0.5074\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.5340 - val_loss: 0.5073\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.5339 - val_loss: 0.5072\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 927us/step - loss: 0.5336 - val_loss: 0.5071\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.5332 - val_loss: 0.5069\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 907us/step - loss: 0.5330 - val_loss: 0.5067\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 923us/step - loss: 0.5327 - val_loss: 0.5066\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.5326 - val_loss: 0.5065\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.5322 - val_loss: 0.5063\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1000us/step - loss: 0.5320 - val_loss: 0.5061\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.5320 - val_loss: 0.5061\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.5318 - val_loss: 0.5060\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.5316 - val_loss: 0.5058\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.5313 - val_loss: 0.5057\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.5311 - val_loss: 0.5055\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.5310 - val_loss: 0.5055\n",
      "121/121 [==============================] - 0s 600us/step - loss: 0.5415\n",
      "[CV] END learning_rate=0.00042591333008443395, n_hidden=0, n_neurons=84; total time=  22.5s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3515 - val_loss: 5.0864\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 4.0445 - val_loss: 3.3615\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 897us/step - loss: 2.7130 - val_loss: 2.3592\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 1.9194 - val_loss: 1.7569\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 1.4326 - val_loss: 1.3880\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 1.1282 - val_loss: 1.1573\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 907us/step - loss: 0.9339 - val_loss: 1.0104\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 903us/step - loss: 0.8078 - val_loss: 0.9157\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.7250 - val_loss: 0.8538\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 0.6698 - val_loss: 0.8126\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.6325 - val_loss: 0.7851\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.6071 - val_loss: 0.7664\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.5894 - val_loss: 0.7535\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.5770 - val_loss: 0.7442\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.5681 - val_loss: 0.7375\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.5615 - val_loss: 0.7325\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 864us/step - loss: 0.5565 - val_loss: 0.7285\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 923us/step - loss: 0.5526 - val_loss: 0.7252\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.5495 - val_loss: 0.7227\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 845us/step - loss: 0.5469 - val_loss: 0.7204\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 945us/step - loss: 0.5447 - val_loss: 0.7185\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.5428 - val_loss: 0.7166\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 0.5411 - val_loss: 0.7149\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 0.5395 - val_loss: 0.7132\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.5381 - val_loss: 0.7116\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 912us/step - loss: 0.5368 - val_loss: 0.7102\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.5355 - val_loss: 0.7088\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 931us/step - loss: 0.5343 - val_loss: 0.7075\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 903us/step - loss: 0.5331 - val_loss: 0.7062\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.5320 - val_loss: 0.7050\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.5310 - val_loss: 0.7039\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 881us/step - loss: 0.5299 - val_loss: 0.7027\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.5290 - val_loss: 0.7016\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 902us/step - loss: 0.5280 - val_loss: 0.7004\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.5271 - val_loss: 0.6993\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 871us/step - loss: 0.5262 - val_loss: 0.6983\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 919us/step - loss: 0.5254 - val_loss: 0.6973\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.5245 - val_loss: 0.6963\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.5237 - val_loss: 0.6952\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 0.5229 - val_loss: 0.6944\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.5222 - val_loss: 0.6935\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.5214 - val_loss: 0.6925\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 911us/step - loss: 0.5207 - val_loss: 0.6916\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 993us/step - loss: 0.5200 - val_loss: 0.6908\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.5194 - val_loss: 0.6899\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.5187 - val_loss: 0.6892\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.5181 - val_loss: 0.6883\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 886us/step - loss: 0.5175 - val_loss: 0.6875\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 920us/step - loss: 0.5169 - val_loss: 0.6868\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.5163 - val_loss: 0.6861\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.5157 - val_loss: 0.6854\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 902us/step - loss: 0.5152 - val_loss: 0.6847\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.5147 - val_loss: 0.6840\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.5142 - val_loss: 0.6834\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 897us/step - loss: 0.5137 - val_loss: 0.6828\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 858us/step - loss: 0.5132 - val_loss: 0.6821\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.5127 - val_loss: 0.6815\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.5123 - val_loss: 0.6809\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.5119 - val_loss: 0.6802\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.5114 - val_loss: 0.6795\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.5110 - val_loss: 0.6790\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.5106 - val_loss: 0.6784\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 845us/step - loss: 0.5102 - val_loss: 0.6779\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 944us/step - loss: 0.5099 - val_loss: 0.6774\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 0.5095 - val_loss: 0.6770\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 898us/step - loss: 0.5091 - val_loss: 0.6765\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 883us/step - loss: 0.5088 - val_loss: 0.6761\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 924us/step - loss: 0.5085 - val_loss: 0.6756\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.5082 - val_loss: 0.6751\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 877us/step - loss: 0.5078 - val_loss: 0.6746\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.5075 - val_loss: 0.6742\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.5072 - val_loss: 0.6737\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 879us/step - loss: 0.5070 - val_loss: 0.6733\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 886us/step - loss: 0.5067 - val_loss: 0.6728\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.5064 - val_loss: 0.6725\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 881us/step - loss: 0.5062 - val_loss: 0.6721\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 893us/step - loss: 0.5059 - val_loss: 0.6716\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.5057 - val_loss: 0.6712\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 897us/step - loss: 0.5054 - val_loss: 0.6708\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 898us/step - loss: 0.5052 - val_loss: 0.6705\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.5050 - val_loss: 0.6702\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.5048 - val_loss: 0.6699\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.5045 - val_loss: 0.6697\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 897us/step - loss: 0.5044 - val_loss: 0.6694\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 898us/step - loss: 0.5041 - val_loss: 0.6689\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 910us/step - loss: 0.5039 - val_loss: 0.6686\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.5038 - val_loss: 0.6683\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.5036 - val_loss: 0.6680\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.5034 - val_loss: 0.6676\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 870us/step - loss: 0.5032 - val_loss: 0.6674\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.5031 - val_loss: 0.6670\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 922us/step - loss: 0.5029 - val_loss: 0.6667\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.5027 - val_loss: 0.6664\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.5026 - val_loss: 0.6661\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.5024 - val_loss: 0.6659\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 887us/step - loss: 0.5023 - val_loss: 0.6657\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.5021 - val_loss: 0.6653\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 898us/step - loss: 0.5020 - val_loss: 0.6650\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.5019 - val_loss: 0.6648\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 0.5018 - val_loss: 0.6645\n",
      "121/121 [==============================] - 0s 625us/step - loss: 1.0235\n",
      "[CV] END learning_rate=0.00042591333008443395, n_hidden=0, n_neurons=84; total time=  22.2s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.0854 - val_loss: 3.9868\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 3.4483 - val_loss: 2.7356\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.4410 - val_loss: 1.9616\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 1.8089 - val_loss: 1.4741\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 1.4054 - val_loss: 1.1636\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 944us/step - loss: 1.1451 - val_loss: 0.9639\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 958us/step - loss: 0.9746 - val_loss: 0.8337\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 915us/step - loss: 0.8624 - val_loss: 0.7493\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.7882 - val_loss: 0.6941\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.7383 - val_loss: 0.6574\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 918us/step - loss: 0.7044 - val_loss: 0.6329\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 911us/step - loss: 0.6814 - val_loss: 0.6169\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 894us/step - loss: 0.6651 - val_loss: 0.6056\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 0.6536 - val_loss: 0.5979\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.6451 - val_loss: 0.5923\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.6384 - val_loss: 0.5878\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 907us/step - loss: 0.6336 - val_loss: 0.5847\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 907us/step - loss: 0.6294 - val_loss: 0.5822\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 0.6258 - val_loss: 0.5797\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 897us/step - loss: 0.6229 - val_loss: 0.5774\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 912us/step - loss: 0.6201 - val_loss: 0.5754\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 0.6176 - val_loss: 0.5734\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 903us/step - loss: 0.6154 - val_loss: 0.5718\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.6130 - val_loss: 0.5703\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 889us/step - loss: 0.6109 - val_loss: 0.5686\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 0.6088 - val_loss: 0.5671\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 887us/step - loss: 0.6066 - val_loss: 0.5652\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.6050 - val_loss: 0.5637\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 904us/step - loss: 0.6029 - val_loss: 0.5620\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 914us/step - loss: 0.6014 - val_loss: 0.5605\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 883us/step - loss: 0.5994 - val_loss: 0.5589\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 882us/step - loss: 0.5978 - val_loss: 0.5573\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 911us/step - loss: 0.5961 - val_loss: 0.5558\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.5946 - val_loss: 0.5545\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 890us/step - loss: 0.5930 - val_loss: 0.5532\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.5916 - val_loss: 0.5519\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 887us/step - loss: 0.5901 - val_loss: 0.5509\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.5884 - val_loss: 0.5497\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 907us/step - loss: 0.5871 - val_loss: 0.5485\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.5856 - val_loss: 0.5472\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.5841 - val_loss: 0.5458\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.5830 - val_loss: 0.5448\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 887us/step - loss: 0.5817 - val_loss: 0.5436\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 891us/step - loss: 0.5804 - val_loss: 0.5425\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 903us/step - loss: 0.5790 - val_loss: 0.5413\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 890us/step - loss: 0.5779 - val_loss: 0.5403\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 904us/step - loss: 0.5769 - val_loss: 0.5394\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 891us/step - loss: 0.5755 - val_loss: 0.5383\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 889us/step - loss: 0.5745 - val_loss: 0.5372\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 0.5734 - val_loss: 0.5364\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.5724 - val_loss: 0.5356\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.5712 - val_loss: 0.5347\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 0.5704 - val_loss: 0.5339\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 902us/step - loss: 0.5694 - val_loss: 0.5331\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 894us/step - loss: 0.5684 - val_loss: 0.5322\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 891us/step - loss: 0.5672 - val_loss: 0.5313\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 891us/step - loss: 0.5664 - val_loss: 0.5303\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.5655 - val_loss: 0.5296\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 902us/step - loss: 0.5647 - val_loss: 0.5290\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.5637 - val_loss: 0.5281\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.5629 - val_loss: 0.5272\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 0.5621 - val_loss: 0.5267\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 903us/step - loss: 0.5614 - val_loss: 0.5260\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 907us/step - loss: 0.5605 - val_loss: 0.5252\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.5599 - val_loss: 0.5248\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 890us/step - loss: 0.5591 - val_loss: 0.5241\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.5583 - val_loss: 0.5233\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 918us/step - loss: 0.5576 - val_loss: 0.5227\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 903us/step - loss: 0.5570 - val_loss: 0.5223\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.5561 - val_loss: 0.5215\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.5555 - val_loss: 0.5208\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.5548 - val_loss: 0.5203\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 0.5544 - val_loss: 0.5199\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 898us/step - loss: 0.5537 - val_loss: 0.5194\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.5531 - val_loss: 0.5190\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.5525 - val_loss: 0.5184\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.5519 - val_loss: 0.5179\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 924us/step - loss: 0.5513 - val_loss: 0.5173\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.5508 - val_loss: 0.5170\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.5502 - val_loss: 0.5167\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 894us/step - loss: 0.5497 - val_loss: 0.5162\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 894us/step - loss: 0.5491 - val_loss: 0.5156\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 916us/step - loss: 0.5487 - val_loss: 0.5151\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 907us/step - loss: 0.5483 - val_loss: 0.5146\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 914us/step - loss: 0.5478 - val_loss: 0.5142\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 0.5472 - val_loss: 0.5140\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 904us/step - loss: 0.5468 - val_loss: 0.5135\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 881us/step - loss: 0.5463 - val_loss: 0.5130\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 939us/step - loss: 0.5460 - val_loss: 0.5127\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 0.5456 - val_loss: 0.5124\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 886us/step - loss: 0.5451 - val_loss: 0.5120\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 889us/step - loss: 0.5447 - val_loss: 0.5116\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 0.5442 - val_loss: 0.5112\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 914us/step - loss: 0.5440 - val_loss: 0.5109\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 0.5436 - val_loss: 0.5106\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 919us/step - loss: 0.5431 - val_loss: 0.5102\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.5427 - val_loss: 0.5100\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 906us/step - loss: 0.5424 - val_loss: 0.5096\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 891us/step - loss: 0.5420 - val_loss: 0.5093\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.5415 - val_loss: 0.5089\n",
      "121/121 [==============================] - 0s 592us/step - loss: 0.5347\n",
      "[CV] END learning_rate=0.00042591333008443395, n_hidden=0, n_neurons=84; total time=  22.3s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2964 - val_loss: 0.6847\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 995us/step - loss: 0.6197 - val_loss: 0.5981\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.5466 - val_loss: 0.5476\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.4987 - val_loss: 0.5131\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 976us/step - loss: 0.4697 - val_loss: 0.4935\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.4498 - val_loss: 0.4843\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 974us/step - loss: 0.4344 - val_loss: 0.4712\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 995us/step - loss: 0.4228 - val_loss: 0.4626\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 970us/step - loss: 0.4140 - val_loss: 0.4601\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.4063 - val_loss: 0.4488\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 0.3998 - val_loss: 0.4500\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.3939 - val_loss: 0.4458\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3883 - val_loss: 0.4375\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.3851 - val_loss: 0.4402\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 0.3804 - val_loss: 0.4324\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 970us/step - loss: 0.3771 - val_loss: 0.4352\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.3735 - val_loss: 0.4304\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.3706 - val_loss: 0.4304\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 991us/step - loss: 0.3674 - val_loss: 0.4265\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.3649 - val_loss: 0.4238\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 980us/step - loss: 0.3624 - val_loss: 0.4276\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 990us/step - loss: 0.3602 - val_loss: 0.4243\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.3583 - val_loss: 0.4252\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 978us/step - loss: 0.3573 - val_loss: 0.4205\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 966us/step - loss: 0.3541 - val_loss: 0.4209\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.3544 - val_loss: 0.4186\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.3537 - val_loss: 0.4238\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 989us/step - loss: 0.3558 - val_loss: 0.4208\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3480 - val_loss: 0.4223\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3522 - val_loss: 0.4204\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.3510 - val_loss: 0.4203\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.3500 - val_loss: 0.4158\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.3479 - val_loss: 0.4213\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.3485 - val_loss: 0.4112\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 964us/step - loss: 0.3415 - val_loss: 0.4154\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3411 - val_loss: 0.4108\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.3385 - val_loss: 0.4102\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 994us/step - loss: 0.3369 - val_loss: 0.4152\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 990us/step - loss: 0.3371 - val_loss: 0.4094\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3379 - val_loss: 0.4152\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.3401 - val_loss: 0.4119\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.3368 - val_loss: 0.4139\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 978us/step - loss: 0.3353 - val_loss: 0.4096\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 974us/step - loss: 0.3341 - val_loss: 0.4142\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 977us/step - loss: 0.3388 - val_loss: 0.4152\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 969us/step - loss: 0.3386 - val_loss: 0.4235\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.3424 - val_loss: 0.4080\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.3347 - val_loss: 0.4139\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.3320 - val_loss: 0.4053\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 0.3295 - val_loss: 0.4100\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.3285 - val_loss: 0.4141\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.3262 - val_loss: 0.4100\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.3263 - val_loss: 0.4073\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 966us/step - loss: 0.3250 - val_loss: 0.4089\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.3240 - val_loss: 0.4104\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.3246 - val_loss: 0.4063\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.3231 - val_loss: 0.4101\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 974us/step - loss: 0.3237 - val_loss: 0.4043\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.3216 - val_loss: 0.4065\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 986us/step - loss: 0.3209 - val_loss: 0.4085\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 970us/step - loss: 0.3211 - val_loss: 0.4065\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 0.3196 - val_loss: 0.4085\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 977us/step - loss: 0.3204 - val_loss: 0.4060\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 998us/step - loss: 0.3193 - val_loss: 0.4114\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.3198 - val_loss: 0.4054\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 978us/step - loss: 0.3197 - val_loss: 0.4140\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 968us/step - loss: 0.3194 - val_loss: 0.4059\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.3176 - val_loss: 0.4122\n",
      "121/121 [==============================] - 0s 625us/step - loss: 0.3460\n",
      "[CV] END learning_rate=0.0028887005224173742, n_hidden=2, n_neurons=24; total time=  16.6s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.5511 - val_loss: 0.8173\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.7311 - val_loss: 0.6973\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 0.6343 - val_loss: 0.6247\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.5638 - val_loss: 0.5790\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 974us/step - loss: 0.5148 - val_loss: 0.5485\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.4843 - val_loss: 0.5260\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.4650 - val_loss: 0.5104\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.4503 - val_loss: 0.4909\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 982us/step - loss: 0.4389 - val_loss: 0.4809\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.4300 - val_loss: 0.4694\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 978us/step - loss: 0.4214 - val_loss: 0.4610\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.4149 - val_loss: 0.4626\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.4104 - val_loss: 0.4537\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.4063 - val_loss: 0.4561\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.4023 - val_loss: 0.4475\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 987us/step - loss: 0.3986 - val_loss: 0.4450\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.3960 - val_loss: 0.4422\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.3935 - val_loss: 0.4449\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.3905 - val_loss: 0.4417\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3888 - val_loss: 0.4423\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 976us/step - loss: 0.3874 - val_loss: 0.4408\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3851 - val_loss: 0.4376\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3834 - val_loss: 0.4384\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 0.3823 - val_loss: 0.4368\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 987us/step - loss: 0.3805 - val_loss: 0.4366\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.3792 - val_loss: 0.4326\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 995us/step - loss: 0.3770 - val_loss: 0.4299\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 0.3763 - val_loss: 0.4303\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.3749 - val_loss: 0.4299\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.3744 - val_loss: 0.4297\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 980us/step - loss: 0.3729 - val_loss: 0.4278\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 974us/step - loss: 0.3718 - val_loss: 0.4278\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.3713 - val_loss: 0.4315\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.3699 - val_loss: 0.4239\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.3690 - val_loss: 0.4238\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 980us/step - loss: 0.3681 - val_loss: 0.4283\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3675 - val_loss: 0.4235\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.3663 - val_loss: 0.4266\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.3655 - val_loss: 0.4189\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3650 - val_loss: 0.4216\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 964us/step - loss: 0.3637 - val_loss: 0.4252\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 974us/step - loss: 0.3631 - val_loss: 0.4213\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 970us/step - loss: 0.3623 - val_loss: 0.4230\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.3620 - val_loss: 0.4223\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3614 - val_loss: 0.4231\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 997us/step - loss: 0.3610 - val_loss: 0.4171\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.3596 - val_loss: 0.4162\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3591 - val_loss: 0.4212\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 974us/step - loss: 0.3587 - val_loss: 0.4195\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 980us/step - loss: 0.3589 - val_loss: 0.4176\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.3566 - val_loss: 0.4358\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 0.3576 - val_loss: 0.4207\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.3567 - val_loss: 0.4165\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3556 - val_loss: 0.4208\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.3550 - val_loss: 0.4186\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.3542 - val_loss: 0.4130\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3537 - val_loss: 0.4139\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.3536 - val_loss: 0.4146\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.3528 - val_loss: 0.4156\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.3521 - val_loss: 0.4155\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 968us/step - loss: 0.3512 - val_loss: 0.4177\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.3511 - val_loss: 0.4135\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3500 - val_loss: 0.4169\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.3503 - val_loss: 0.4160\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.3492 - val_loss: 0.4147\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.3493 - val_loss: 0.4138\n",
      "121/121 [==============================] - 0s 627us/step - loss: 0.3725\n",
      "[CV] END learning_rate=0.0028887005224173742, n_hidden=2, n_neurons=24; total time=  16.1s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.9688 - val_loss: 0.8859\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7772 - val_loss: 0.6548\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.6236 - val_loss: 0.5930\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.5676 - val_loss: 0.5607\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 0.5273 - val_loss: 0.5359\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.4975 - val_loss: 0.5138\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.4727 - val_loss: 0.5006\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.4544 - val_loss: 0.4834\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 969us/step - loss: 0.4396 - val_loss: 0.4765\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.4293 - val_loss: 0.4666\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4197 - val_loss: 0.4677\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.4140 - val_loss: 0.4600\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.4069 - val_loss: 0.4553\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.4022 - val_loss: 0.4571\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.3994 - val_loss: 0.4478\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.3943 - val_loss: 0.4460\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.3934 - val_loss: 0.4454\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3891 - val_loss: 0.4453\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.3872 - val_loss: 0.4433\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.3845 - val_loss: 0.4412\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 0.3831 - val_loss: 0.4387\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3817 - val_loss: 0.4400\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 987us/step - loss: 0.3795 - val_loss: 0.4385\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.3772 - val_loss: 0.4423\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.3777 - val_loss: 0.4377\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3750 - val_loss: 0.4327\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.3733 - val_loss: 0.4330\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.3748 - val_loss: 0.4370\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.3729 - val_loss: 0.4349\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 982us/step - loss: 0.3737 - val_loss: 0.4341\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.3696 - val_loss: 0.4323\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.3691 - val_loss: 0.4323\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.3675 - val_loss: 0.4297\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3670 - val_loss: 0.4326\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1000us/step - loss: 0.3676 - val_loss: 0.4316\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3645 - val_loss: 0.4273\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3652 - val_loss: 0.4298\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.3640 - val_loss: 0.4338\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 977us/step - loss: 0.3663 - val_loss: 0.4291\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3627 - val_loss: 0.4296\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3631 - val_loss: 0.4273\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3603 - val_loss: 0.4281\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3604 - val_loss: 0.4270\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.3592 - val_loss: 0.4257\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 964us/step - loss: 0.3604 - val_loss: 0.4296\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 987us/step - loss: 0.3574 - val_loss: 0.4244\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3576 - val_loss: 0.4240\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3575 - val_loss: 0.4220\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 970us/step - loss: 0.3557 - val_loss: 0.4255\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1000us/step - loss: 0.3560 - val_loss: 0.4215\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 980us/step - loss: 0.3540 - val_loss: 0.4247\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.3545 - val_loss: 0.4199\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.3541 - val_loss: 0.4203\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3532 - val_loss: 0.4214\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.3522 - val_loss: 0.4209\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.3509 - val_loss: 0.4194\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.3513 - val_loss: 0.4201\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3513 - val_loss: 0.4190\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3497 - val_loss: 0.4186\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.3497 - val_loss: 0.4194\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3489 - val_loss: 0.4212\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3478 - val_loss: 0.4207\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3489 - val_loss: 0.4181\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 993us/step - loss: 0.3478 - val_loss: 0.4210\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.3479 - val_loss: 0.4177\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3474 - val_loss: 0.4248\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.3491 - val_loss: 0.4200\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3493 - val_loss: 0.4278\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.3534 - val_loss: 0.4273\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3610 - val_loss: 0.4365\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 980us/step - loss: 0.3685 - val_loss: 0.4325\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.3675 - val_loss: 0.4306\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.3589 - val_loss: 0.4274\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.3520 - val_loss: 0.4448\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 969us/step - loss: 0.3489 - val_loss: 0.4153\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 977us/step - loss: 0.3435 - val_loss: 0.4212\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.3470 - val_loss: 0.4165\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 994us/step - loss: 0.3426 - val_loss: 0.4165\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.3426 - val_loss: 0.4146\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.3417 - val_loss: 0.4167\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 993us/step - loss: 0.3412 - val_loss: 0.4152\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.3409 - val_loss: 0.4168\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.3409 - val_loss: 0.4176\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.3400 - val_loss: 0.4134\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.3401 - val_loss: 0.4160\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.3401 - val_loss: 0.4142\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.3395 - val_loss: 0.4143\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3390 - val_loss: 0.4142\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3378 - val_loss: 0.4166\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3382 - val_loss: 0.4138\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3375 - val_loss: 0.4176\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3391 - val_loss: 0.4139\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3406 - val_loss: 0.4217\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3462 - val_loss: 0.4151\n",
      "121/121 [==============================] - 0s 608us/step - loss: 0.3408\n",
      "[CV] END learning_rate=0.0028887005224173742, n_hidden=2, n_neurons=24; total time=  23.1s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 6.5760 - val_loss: 3.9316\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 3.1285 - val_loss: 2.2473\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 1.9932 - val_loss: 1.5649\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 954us/step - loss: 1.5126 - val_loss: 1.2498\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 1.2695 - val_loss: 1.0784\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 929us/step - loss: 1.1229 - val_loss: 0.9736\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 1.0265 - val_loss: 0.9032\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 956us/step - loss: 0.9584 - val_loss: 0.8530\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 928us/step - loss: 0.9086 - val_loss: 0.8158\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.8704 - val_loss: 0.7868\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 0.8406 - val_loss: 0.7644\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 0.8172 - val_loss: 0.7468\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 927us/step - loss: 0.7977 - val_loss: 0.7316\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 929us/step - loss: 0.7817 - val_loss: 0.7195\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 926us/step - loss: 0.7685 - val_loss: 0.7094\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.7564 - val_loss: 0.7002\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 0.7460 - val_loss: 0.6923\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.7367 - val_loss: 0.6852\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.7287 - val_loss: 0.6791\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 932us/step - loss: 0.7215 - val_loss: 0.6733\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.7151 - val_loss: 0.6686\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.7087 - val_loss: 0.6637\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 970us/step - loss: 0.7030 - val_loss: 0.6593\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 936us/step - loss: 0.6977 - val_loss: 0.6550\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.6928 - val_loss: 0.6514\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.6880 - val_loss: 0.6480\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.6831 - val_loss: 0.6442\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.6792 - val_loss: 0.6414\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.6747 - val_loss: 0.6382\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.6708 - val_loss: 0.6352\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 954us/step - loss: 0.6671 - val_loss: 0.6324\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 929us/step - loss: 0.6635 - val_loss: 0.6300\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 929us/step - loss: 0.6598 - val_loss: 0.6270\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.6566 - val_loss: 0.6243\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.6534 - val_loss: 0.6222\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 929us/step - loss: 0.6501 - val_loss: 0.6191\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 0.6473 - val_loss: 0.6176\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.6441 - val_loss: 0.6145\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 0.6415 - val_loss: 0.6126\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.6387 - val_loss: 0.6105\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.6360 - val_loss: 0.6080\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.6336 - val_loss: 0.6062\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 0.6311 - val_loss: 0.6042\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.6285 - val_loss: 0.6018\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 929us/step - loss: 0.6263 - val_loss: 0.5998\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.6241 - val_loss: 0.5977\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 966us/step - loss: 0.6218 - val_loss: 0.5956\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.6197 - val_loss: 0.5938\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.6176 - val_loss: 0.5921\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: 0.6153 - val_loss: 0.5900\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 931us/step - loss: 0.6136 - val_loss: 0.5881\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.6116 - val_loss: 0.5865\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6096 - val_loss: 0.5849\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 997us/step - loss: 0.6078 - val_loss: 0.5833\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.6057 - val_loss: 0.5813\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.6041 - val_loss: 0.5800\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 954us/step - loss: 0.6020 - val_loss: 0.5778\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.6004 - val_loss: 0.5764\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.5988 - val_loss: 0.5749\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 958us/step - loss: 0.5970 - val_loss: 0.5733\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5952 - val_loss: 0.5712\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.5935 - val_loss: 0.5694\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 929us/step - loss: 0.5918 - val_loss: 0.5677\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.5903 - val_loss: 0.5663\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 0.5885 - val_loss: 0.5648\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 958us/step - loss: 0.5869 - val_loss: 0.5631\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.5853 - val_loss: 0.5614\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 929us/step - loss: 0.5838 - val_loss: 0.5604\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5819 - val_loss: 0.5584\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.5804 - val_loss: 0.5566\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5788 - val_loss: 0.5549\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.5775 - val_loss: 0.5536\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 0.5760 - val_loss: 0.5522\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 910us/step - loss: 0.5743 - val_loss: 0.5505\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5730 - val_loss: 0.5491\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.5712 - val_loss: 0.5472\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 910us/step - loss: 0.5701 - val_loss: 0.5463\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 919us/step - loss: 0.5683 - val_loss: 0.5445\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.5668 - val_loss: 0.5430\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: 0.5655 - val_loss: 0.5414\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5638 - val_loss: 0.5397\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.5627 - val_loss: 0.5389\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 916us/step - loss: 0.5612 - val_loss: 0.5378\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5596 - val_loss: 0.5364\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.5581 - val_loss: 0.5344\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.5567 - val_loss: 0.5331\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.5554 - val_loss: 0.5324\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 914us/step - loss: 0.5540 - val_loss: 0.5307\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 862us/step - loss: 0.5523 - val_loss: 0.5287\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.5510 - val_loss: 0.5272\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5495 - val_loss: 0.5256\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.5483 - val_loss: 0.5249\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 893us/step - loss: 0.5464 - val_loss: 0.5228\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.5453 - val_loss: 0.5212\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 945us/step - loss: 0.5441 - val_loss: 0.5205\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5425 - val_loss: 0.5197\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.5411 - val_loss: 0.5183\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 868us/step - loss: 0.5398 - val_loss: 0.5166\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 883us/step - loss: 0.5384 - val_loss: 0.5153\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 894us/step - loss: 0.5372 - val_loss: 0.5141\n",
      "121/121 [==============================] - 0s 636us/step - loss: 0.5376\n",
      "[CV] END learning_rate=0.00046349612360143094, n_hidden=1, n_neurons=4; total time=  23.6s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2506 - val_loss: 3.1105\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 2.6406 - val_loss: 2.1567\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 918us/step - loss: 1.9367 - val_loss: 1.7167\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.5668 - val_loss: 1.4497\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 970us/step - loss: 1.3126 - val_loss: 1.2403\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 1.1011 - val_loss: 1.0580\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 927us/step - loss: 0.9228 - val_loss: 0.9105\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: 0.7921 - val_loss: 0.8083\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 897us/step - loss: 0.7060 - val_loss: 0.7431\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6538 - val_loss: 0.7035\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6221 - val_loss: 0.6786\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 919us/step - loss: 0.6018 - val_loss: 0.6621\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 890us/step - loss: 0.5880 - val_loss: 0.6505\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.5779 - val_loss: 0.6418\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 0.5700 - val_loss: 0.6348\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 920us/step - loss: 0.5634 - val_loss: 0.6291\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 0.5575 - val_loss: 0.6239\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.5523 - val_loss: 0.6193\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 949us/step - loss: 0.5476 - val_loss: 0.6152\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 0.5434 - val_loss: 0.6117\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.5396 - val_loss: 0.6082\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5360 - val_loss: 0.6051\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 0.5327 - val_loss: 0.6022\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 929us/step - loss: 0.5296 - val_loss: 0.5995\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 999us/step - loss: 0.5268 - val_loss: 0.5967\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.5241 - val_loss: 0.5941\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 936us/step - loss: 0.5215 - val_loss: 0.5917\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5191 - val_loss: 0.5892\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 914us/step - loss: 0.5168 - val_loss: 0.5872\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.5147 - val_loss: 0.5848\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: 0.5126 - val_loss: 0.5824\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5106 - val_loss: 0.5803\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 969us/step - loss: 0.5087 - val_loss: 0.5782\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5068 - val_loss: 0.5763\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5050 - val_loss: 0.5743\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 933us/step - loss: 0.5033 - val_loss: 0.5721\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 995us/step - loss: 0.5016 - val_loss: 0.5702\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5000 - val_loss: 0.5681\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1000us/step - loss: 0.4984 - val_loss: 0.5660\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4968 - val_loss: 0.5642\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 962us/step - loss: 0.4953 - val_loss: 0.5625\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4938 - val_loss: 0.5606\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4923 - val_loss: 0.5584\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4909 - val_loss: 0.5565\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4895 - val_loss: 0.5548\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4881 - val_loss: 0.5526\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.4868 - val_loss: 0.5510\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 924us/step - loss: 0.4855 - val_loss: 0.5493\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4842 - val_loss: 0.5475\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.4829 - val_loss: 0.5455\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 902us/step - loss: 0.4817 - val_loss: 0.5435\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 914us/step - loss: 0.4805 - val_loss: 0.5419\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.4793 - val_loss: 0.5401\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4781 - val_loss: 0.5386\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 964us/step - loss: 0.4770 - val_loss: 0.5370\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 863us/step - loss: 0.4759 - val_loss: 0.5353\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.4747 - val_loss: 0.5333\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4737 - val_loss: 0.5320\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.4726 - val_loss: 0.5304\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 932us/step - loss: 0.4716 - val_loss: 0.5286\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 922us/step - loss: 0.4706 - val_loss: 0.5272\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 958us/step - loss: 0.4695 - val_loss: 0.5258\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 933us/step - loss: 0.4685 - val_loss: 0.5241\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 918us/step - loss: 0.4676 - val_loss: 0.5226\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 887us/step - loss: 0.4666 - val_loss: 0.5215\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 997us/step - loss: 0.4657 - val_loss: 0.5202\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 924us/step - loss: 0.4648 - val_loss: 0.5186\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 910us/step - loss: 0.4639 - val_loss: 0.5174\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 926us/step - loss: 0.4630 - val_loss: 0.5160\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 862us/step - loss: 0.4621 - val_loss: 0.5143\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 919us/step - loss: 0.4613 - val_loss: 0.5128\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 878us/step - loss: 0.4604 - val_loss: 0.5119\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4596 - val_loss: 0.5105\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4588 - val_loss: 0.5092\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: 0.4580 - val_loss: 0.5081\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 916us/step - loss: 0.4572 - val_loss: 0.5069\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 882us/step - loss: 0.4564 - val_loss: 0.5055\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4557 - val_loss: 0.5045\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 890us/step - loss: 0.4550 - val_loss: 0.5034\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 927us/step - loss: 0.4543 - val_loss: 0.5023\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 929us/step - loss: 0.4535 - val_loss: 0.5013\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4529 - val_loss: 0.5001\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.4522 - val_loss: 0.4992\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 0.4515 - val_loss: 0.4980\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 929us/step - loss: 0.4509 - val_loss: 0.4971\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.4503 - val_loss: 0.4960\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 954us/step - loss: 0.4497 - val_loss: 0.4951\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4490 - val_loss: 0.4941\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 974us/step - loss: 0.4484 - val_loss: 0.4932\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.4478 - val_loss: 0.4921\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4472 - val_loss: 0.4909\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 915us/step - loss: 0.4466 - val_loss: 0.4900\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.4460 - val_loss: 0.4891\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 860us/step - loss: 0.4454 - val_loss: 0.4882\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 968us/step - loss: 0.4449 - val_loss: 0.4874\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 935us/step - loss: 0.4444 - val_loss: 0.4866\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 945us/step - loss: 0.4438 - val_loss: 0.4855\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 923us/step - loss: 0.4433 - val_loss: 0.4845\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 923us/step - loss: 0.4428 - val_loss: 0.4838\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4424 - val_loss: 0.4830\n",
      "121/121 [==============================] - 0s 583us/step - loss: 0.4625\n",
      "[CV] END learning_rate=0.00046349612360143094, n_hidden=1, n_neurons=4; total time=  23.4s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 7.6090 - val_loss: 4.3580\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 3.2771 - val_loss: 2.2595\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 1.9202 - val_loss: 1.4633\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 932us/step - loss: 1.3488 - val_loss: 1.1085\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 924us/step - loss: 1.0853 - val_loss: 0.9496\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.9431 - val_loss: 0.8586\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.8557 - val_loss: 0.8031\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.7985 - val_loss: 0.7667\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 922us/step - loss: 0.7587 - val_loss: 0.7414\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 920us/step - loss: 0.7299 - val_loss: 0.7222\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 949us/step - loss: 0.7080 - val_loss: 0.7079\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 912us/step - loss: 0.6911 - val_loss: 0.6967\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.6777 - val_loss: 0.6878\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 926us/step - loss: 0.6672 - val_loss: 0.6811\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 923us/step - loss: 0.6584 - val_loss: 0.6755\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 855us/step - loss: 0.6511 - val_loss: 0.6707\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 0.6448 - val_loss: 0.6663\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.6392 - val_loss: 0.6626\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 920us/step - loss: 0.6342 - val_loss: 0.6592\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.6297 - val_loss: 0.6562\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 859us/step - loss: 0.6254 - val_loss: 0.6535\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 916us/step - loss: 0.6215 - val_loss: 0.6505\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.6179 - val_loss: 0.6478\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.6143 - val_loss: 0.6452\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 0.6109 - val_loss: 0.6425\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 868us/step - loss: 0.6076 - val_loss: 0.6397\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 910us/step - loss: 0.6042 - val_loss: 0.6368\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 903us/step - loss: 0.6010 - val_loss: 0.6341\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 871us/step - loss: 0.5978 - val_loss: 0.6314\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 962us/step - loss: 0.5946 - val_loss: 0.6286\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 986us/step - loss: 0.5915 - val_loss: 0.6258\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.5885 - val_loss: 0.6233\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 916us/step - loss: 0.5854 - val_loss: 0.6206\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 919us/step - loss: 0.5824 - val_loss: 0.6180\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 962us/step - loss: 0.5794 - val_loss: 0.6153\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 873us/step - loss: 0.5765 - val_loss: 0.6130\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.5736 - val_loss: 0.6103\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.5707 - val_loss: 0.6074\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5679 - val_loss: 0.6046\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.5651 - val_loss: 0.6019\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5624 - val_loss: 0.5994\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.5596 - val_loss: 0.5969\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 874us/step - loss: 0.5569 - val_loss: 0.5942\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.5542 - val_loss: 0.5916\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5515 - val_loss: 0.5890\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 974us/step - loss: 0.5489 - val_loss: 0.5864\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 904us/step - loss: 0.5464 - val_loss: 0.5841\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5439 - val_loss: 0.5816\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 915us/step - loss: 0.5414 - val_loss: 0.5790\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.5391 - val_loss: 0.5765\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1000us/step - loss: 0.5367 - val_loss: 0.5740\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 873us/step - loss: 0.5344 - val_loss: 0.5717\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.5321 - val_loss: 0.5694\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.5298 - val_loss: 0.5672\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 922us/step - loss: 0.5276 - val_loss: 0.5650\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 881us/step - loss: 0.5253 - val_loss: 0.5629\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.5232 - val_loss: 0.5603\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.5211 - val_loss: 0.5581\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 929us/step - loss: 0.5190 - val_loss: 0.5558\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.5170 - val_loss: 0.5535\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 893us/step - loss: 0.5150 - val_loss: 0.5516\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.5131 - val_loss: 0.5494\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 873us/step - loss: 0.5113 - val_loss: 0.5473\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 969us/step - loss: 0.5094 - val_loss: 0.5452\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.5076 - val_loss: 0.5432\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 978us/step - loss: 0.5059 - val_loss: 0.5413\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.5042 - val_loss: 0.5392\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 903us/step - loss: 0.5025 - val_loss: 0.5372\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 894us/step - loss: 0.5009 - val_loss: 0.5353\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.4993 - val_loss: 0.5333\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 857us/step - loss: 0.4977 - val_loss: 0.5314\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 867us/step - loss: 0.4962 - val_loss: 0.5296\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.4947 - val_loss: 0.5278\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4933 - val_loss: 0.5261\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.4919 - val_loss: 0.5244\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 911us/step - loss: 0.4905 - val_loss: 0.5227\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.4891 - val_loss: 0.5210\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 904us/step - loss: 0.4877 - val_loss: 0.5193\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.4865 - val_loss: 0.5178\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4853 - val_loss: 0.5162\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 841us/step - loss: 0.4840 - val_loss: 0.5147\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 846us/step - loss: 0.4828 - val_loss: 0.5132\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 856us/step - loss: 0.4817 - val_loss: 0.5117\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 931us/step - loss: 0.4805 - val_loss: 0.5102\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 956us/step - loss: 0.4794 - val_loss: 0.5088\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 894us/step - loss: 0.4783 - val_loss: 0.5074\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 894us/step - loss: 0.4773 - val_loss: 0.5061\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4762 - val_loss: 0.5048\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 962us/step - loss: 0.4752 - val_loss: 0.5036\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 911us/step - loss: 0.4743 - val_loss: 0.5023\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4733 - val_loss: 0.5010\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4724 - val_loss: 0.4998\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.4714 - val_loss: 0.4986\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 912us/step - loss: 0.4705 - val_loss: 0.4975\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.4696 - val_loss: 0.4964\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 0.4688 - val_loss: 0.4953\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.4679 - val_loss: 0.4941\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 939us/step - loss: 0.4671 - val_loss: 0.4931\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 911us/step - loss: 0.4663 - val_loss: 0.4920\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 0.4656 - val_loss: 0.4910\n",
      "121/121 [==============================] - 0s 547us/step - loss: 0.4623\n",
      "[CV] END learning_rate=0.00046349612360143094, n_hidden=1, n_neurons=4; total time=  22.7s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 1.9028 - val_loss: 1.3276\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 1.3381 - val_loss: 1.3062\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 962us/step - loss: 1.3214 - val_loss: 1.2911\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 962us/step - loss: 1.3069 - val_loss: 1.2710\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 1.2818 - val_loss: 1.2342\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 966us/step - loss: 1.2323 - val_loss: 1.1550\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 1.1346 - val_loss: 1.0044\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9675 - val_loss: 0.8348\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 0.8237 - val_loss: 0.7464\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7256 - val_loss: 0.6742\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6292 - val_loss: 0.5905\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.5328 - val_loss: 0.5359\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.4602 - val_loss: 0.4662\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 969us/step - loss: 0.4270 - val_loss: 0.4571\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4183 - val_loss: 0.4420\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.4132 - val_loss: 0.4463\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4128 - val_loss: 0.4377\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 961us/step - loss: 0.4112 - val_loss: 0.4447\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.4126 - val_loss: 0.4461\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4077 - val_loss: 0.4309\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 968us/step - loss: 0.4064 - val_loss: 0.4473\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 968us/step - loss: 0.4084 - val_loss: 0.4500\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 969us/step - loss: 0.4047 - val_loss: 0.4283\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4040 - val_loss: 0.4382\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.4067 - val_loss: 0.4688\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 969us/step - loss: 0.4082 - val_loss: 0.4464\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 0.4040 - val_loss: 0.4317\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.4020 - val_loss: 0.4423\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.4039 - val_loss: 0.4331\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.4020 - val_loss: 0.4521\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4097 - val_loss: 0.4313\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 903us/step - loss: 0.4022 - val_loss: 0.4346\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 986us/step - loss: 0.4102 - val_loss: 0.4437\n",
      "121/121 [==============================] - 0s 608us/step - loss: 0.4217\n",
      "[CV] END learning_rate=0.007755079455500356, n_hidden=3, n_neurons=2; total time=   8.2s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 1.9387 - val_loss: 1.3147\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 1.3482 - val_loss: 1.3173\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 1.3475 - val_loss: 1.3161\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 1.3472 - val_loss: 1.3172\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 956us/step - loss: 1.3470 - val_loss: 1.3168\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 998us/step - loss: 1.3468 - val_loss: 1.3160\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 1.3464 - val_loss: 1.3186\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 982us/step - loss: 1.3464 - val_loss: 1.3158\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3458 - val_loss: 1.3142\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 1.3450 - val_loss: 1.3127\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 978us/step - loss: 1.3446 - val_loss: 1.3134\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 935us/step - loss: 1.3433 - val_loss: 1.3113\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 1.3414 - val_loss: 1.3085\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3381 - val_loss: 1.3051\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: 1.3308 - val_loss: 1.2900\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 1.3099 - val_loss: 1.2469\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 1.2318 - val_loss: 1.0811\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 1.0299 - val_loss: 0.8583\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.8587 - val_loss: 0.7508\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 935us/step - loss: 0.7301 - val_loss: 0.6632\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 0.6207 - val_loss: 0.6685\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 969us/step - loss: 0.5680 - val_loss: 0.6188\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 924us/step - loss: 0.5389 - val_loss: 0.6129\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 989us/step - loss: 0.5210 - val_loss: 0.5521\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 986us/step - loss: 0.5050 - val_loss: 0.5673\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.4952 - val_loss: 0.5416\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.4800 - val_loss: 0.5725\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.4747 - val_loss: 0.4974\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 964us/step - loss: 0.4579 - val_loss: 0.5068\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4524 - val_loss: 0.5195\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.4451 - val_loss: 0.4831\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.4393 - val_loss: 0.4774\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.4346 - val_loss: 0.5231\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 968us/step - loss: 0.4344 - val_loss: 0.4755\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4289 - val_loss: 0.4726\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 978us/step - loss: 0.4317 - val_loss: 0.4751\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 0.4247 - val_loss: 0.4696\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4279 - val_loss: 0.4812\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 986us/step - loss: 0.4229 - val_loss: 0.4713\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 961us/step - loss: 0.4237 - val_loss: 0.4764\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4198 - val_loss: 0.4698\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4179 - val_loss: 0.4702\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4198 - val_loss: 0.4723\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4191 - val_loss: 0.4773\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4189 - val_loss: 0.4720\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4161 - val_loss: 0.4713\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4158 - val_loss: 0.4714\n",
      "121/121 [==============================] - 0s 657us/step - loss: 0.4451\n",
      "[CV] END learning_rate=0.007755079455500356, n_hidden=3, n_neurons=2; total time=  11.8s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 1.0669 - val_loss: 0.7585\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7105 - val_loss: 0.6837\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6487 - val_loss: 0.6466\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5991 - val_loss: 0.6105\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5564 - val_loss: 0.5706\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5205 - val_loss: 0.5384\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4946 - val_loss: 0.5085\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4769 - val_loss: 0.4890\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4657 - val_loss: 0.4813\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4602 - val_loss: 0.4749\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4562 - val_loss: 0.4697\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4540 - val_loss: 0.4837\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4471 - val_loss: 0.4679\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4477 - val_loss: 0.4657\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4441 - val_loss: 0.4619\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4422 - val_loss: 0.4589\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 998us/step - loss: 0.4414 - val_loss: 0.4607\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4423 - val_loss: 0.4702\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4394 - val_loss: 0.4628\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4394 - val_loss: 0.4652\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4429 - val_loss: 0.4633\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4387 - val_loss: 0.4614\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4415 - val_loss: 0.4613\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4335 - val_loss: 0.4741\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4347 - val_loss: 0.4584\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4315 - val_loss: 0.4569\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4319 - val_loss: 0.4569\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4328 - val_loss: 0.4624\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4326 - val_loss: 0.4590\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4410 - val_loss: 0.4644\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4292 - val_loss: 0.4682\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4295 - val_loss: 0.4566\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4294 - val_loss: 0.4620\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4292 - val_loss: 0.4712\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4289 - val_loss: 0.4694\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4296 - val_loss: 0.4612\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4259 - val_loss: 0.4572\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4261 - val_loss: 0.4664\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4255 - val_loss: 0.4614\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4252 - val_loss: 0.4650\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4232 - val_loss: 0.4590\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4239 - val_loss: 0.4679\n",
      "121/121 [==============================] - 0s 650us/step - loss: 0.4163\n",
      "[CV] END learning_rate=0.007755079455500356, n_hidden=3, n_neurons=2; total time=  11.0s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 2.0403 - val_loss: 0.8713\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7844 - val_loss: 0.6952\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6654 - val_loss: 0.6361\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6056 - val_loss: 0.5970\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5573 - val_loss: 0.5684\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5193 - val_loss: 0.5503\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4895 - val_loss: 0.5263\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4671 - val_loss: 0.5133\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4526 - val_loss: 0.5085\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4423 - val_loss: 0.4984\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4348 - val_loss: 0.4960\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4294 - val_loss: 0.4983\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4252 - val_loss: 0.4877\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4219 - val_loss: 0.4895\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4189 - val_loss: 0.4812\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4161 - val_loss: 0.4836\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4139 - val_loss: 0.4769\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4119 - val_loss: 0.4772\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4095 - val_loss: 0.4752\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4078 - val_loss: 0.4723\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4057 - val_loss: 0.4774\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4046 - val_loss: 0.4723\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4027 - val_loss: 0.4634\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4010 - val_loss: 0.4687\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3995 - val_loss: 0.4630\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3983 - val_loss: 0.4647\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3969 - val_loss: 0.4618\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3956 - val_loss: 0.4623\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3936 - val_loss: 0.4571\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3926 - val_loss: 0.4598\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3913 - val_loss: 0.4563\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3901 - val_loss: 0.4580\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3887 - val_loss: 0.4540\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3874 - val_loss: 0.4506\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3863 - val_loss: 0.4508\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3854 - val_loss: 0.4511\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3842 - val_loss: 0.4464\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3832 - val_loss: 0.4515\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3821 - val_loss: 0.4434\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3812 - val_loss: 0.4459\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3801 - val_loss: 0.4459\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3790 - val_loss: 0.4464\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3786 - val_loss: 0.4455\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3771 - val_loss: 0.4440\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3766 - val_loss: 0.4454\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3754 - val_loss: 0.4442\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3747 - val_loss: 0.4425\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3739 - val_loss: 0.4419\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3730 - val_loss: 0.4404\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3724 - val_loss: 0.4398\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3712 - val_loss: 0.4427\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3709 - val_loss: 0.4403\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3702 - val_loss: 0.4388\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3691 - val_loss: 0.4372\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3676 - val_loss: 0.4393\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3678 - val_loss: 0.4351\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3668 - val_loss: 0.4364\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3656 - val_loss: 0.4325\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3651 - val_loss: 0.4369\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3641 - val_loss: 0.4355\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3639 - val_loss: 0.4347\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3627 - val_loss: 0.4339\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3621 - val_loss: 0.4337\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3615 - val_loss: 0.4359\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3606 - val_loss: 0.4328\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3602 - val_loss: 0.4352\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3591 - val_loss: 0.4339\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3584 - val_loss: 0.4352\n",
      "121/121 [==============================] - 0s 625us/step - loss: 0.3824\n",
      "[CV] END learning_rate=0.002056176266085161, n_hidden=3, n_neurons=10; total time=  17.5s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 1.6858 - val_loss: 0.8553\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8018 - val_loss: 0.7368\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7324 - val_loss: 0.6811\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6925 - val_loss: 0.6452\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6615 - val_loss: 0.6202\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6367 - val_loss: 0.6043\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6143 - val_loss: 0.5829\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5933 - val_loss: 0.5669\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5730 - val_loss: 0.5531\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5536 - val_loss: 0.5400\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5343 - val_loss: 0.5257\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1000us/step - loss: 0.5166 - val_loss: 0.5143\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4991 - val_loss: 0.5022\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4824 - val_loss: 0.4976\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4665 - val_loss: 0.4835\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4518 - val_loss: 0.4788\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4397 - val_loss: 0.4694\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4298 - val_loss: 0.4667\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4213 - val_loss: 0.4600\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4153 - val_loss: 0.4588\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4107 - val_loss: 0.4565\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4067 - val_loss: 0.4523\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4038 - val_loss: 0.4516\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4012 - val_loss: 0.4488\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3990 - val_loss: 0.4467\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3971 - val_loss: 0.4458\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3949 - val_loss: 0.4446\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3934 - val_loss: 0.4428\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3913 - val_loss: 0.4447\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3909 - val_loss: 0.4430\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3891 - val_loss: 0.4411\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3876 - val_loss: 0.4411\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3865 - val_loss: 0.4412\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3855 - val_loss: 0.4391\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3843 - val_loss: 0.4387\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3830 - val_loss: 0.4400\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3821 - val_loss: 0.4389\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3809 - val_loss: 0.4409\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3798 - val_loss: 0.4364\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3791 - val_loss: 0.4381\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3777 - val_loss: 0.4391\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3772 - val_loss: 0.4379\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3760 - val_loss: 0.4379\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3757 - val_loss: 0.4370\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3748 - val_loss: 0.4398\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3744 - val_loss: 0.4358\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3734 - val_loss: 0.4371\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3728 - val_loss: 0.4383\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3721 - val_loss: 0.4362\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3722 - val_loss: 0.4359\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3705 - val_loss: 0.4467\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3710 - val_loss: 0.4355\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3703 - val_loss: 0.4343\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3696 - val_loss: 0.4375\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3692 - val_loss: 0.4359\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3686 - val_loss: 0.4334\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3679 - val_loss: 0.4331\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3679 - val_loss: 0.4343\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3674 - val_loss: 0.4339\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3664 - val_loss: 0.4335\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3658 - val_loss: 0.4332\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3655 - val_loss: 0.4345\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3651 - val_loss: 0.4341\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3649 - val_loss: 0.4330\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3639 - val_loss: 0.4333\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3636 - val_loss: 0.4344\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3636 - val_loss: 0.4327\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3625 - val_loss: 0.4325\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3623 - val_loss: 0.4303\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3617 - val_loss: 0.4307\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3615 - val_loss: 0.4307\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3611 - val_loss: 0.4309\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3605 - val_loss: 0.4290\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3606 - val_loss: 0.4298\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3603 - val_loss: 0.4309\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3594 - val_loss: 0.4307\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3593 - val_loss: 0.4313\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3590 - val_loss: 0.4317\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3588 - val_loss: 0.4289\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3583 - val_loss: 0.4319\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3581 - val_loss: 0.4304\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3577 - val_loss: 0.4295\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3568 - val_loss: 0.4309\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3573 - val_loss: 0.4281\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3566 - val_loss: 0.4289\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3565 - val_loss: 0.4279\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3562 - val_loss: 0.4280\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3556 - val_loss: 0.4325\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3559 - val_loss: 0.4290\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3554 - val_loss: 0.4278\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3552 - val_loss: 0.4277\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3549 - val_loss: 0.4284\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3545 - val_loss: 0.4271\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3543 - val_loss: 0.4251\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3537 - val_loss: 0.4274\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3538 - val_loss: 0.4272\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3533 - val_loss: 0.4238\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3533 - val_loss: 0.4244\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3532 - val_loss: 0.4249\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3529 - val_loss: 0.4243\n",
      "121/121 [==============================] - 0s 658us/step - loss: 0.3697\n",
      "[CV] END learning_rate=0.002056176266085161, n_hidden=3, n_neurons=10; total time=  25.8s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 2.2796 - val_loss: 1.0263\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8120 - val_loss: 0.6953\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6683 - val_loss: 0.6302\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6083 - val_loss: 0.5904\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5633 - val_loss: 0.5566\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5256 - val_loss: 0.5302\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4953 - val_loss: 0.5106\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4713 - val_loss: 0.4917\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4529 - val_loss: 0.4802\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4402 - val_loss: 0.4735\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4297 - val_loss: 0.4674\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4223 - val_loss: 0.4635\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4171 - val_loss: 0.4583\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4128 - val_loss: 0.4598\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4095 - val_loss: 0.4536\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4064 - val_loss: 0.4515\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4036 - val_loss: 0.4514\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4013 - val_loss: 0.4502\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3992 - val_loss: 0.4477\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3973 - val_loss: 0.4471\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3953 - val_loss: 0.4453\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3935 - val_loss: 0.4461\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1000us/step - loss: 0.3920 - val_loss: 0.4450\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3903 - val_loss: 0.4478\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3892 - val_loss: 0.4447\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3877 - val_loss: 0.4417\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3866 - val_loss: 0.4419\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3852 - val_loss: 0.4417\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3843 - val_loss: 0.4426\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3833 - val_loss: 0.4429\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3822 - val_loss: 0.4425\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3812 - val_loss: 0.4429\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3802 - val_loss: 0.4393\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3792 - val_loss: 0.4438\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 999us/step - loss: 0.3787 - val_loss: 0.4431\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 990us/step - loss: 0.3778 - val_loss: 0.4384\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3777 - val_loss: 0.4384\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3764 - val_loss: 0.4421\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3759 - val_loss: 0.4403\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3751 - val_loss: 0.4400\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3741 - val_loss: 0.4396\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3738 - val_loss: 0.4412\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.3731 - val_loss: 0.4383\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3726 - val_loss: 0.4388\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3723 - val_loss: 0.4415\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3715 - val_loss: 0.4373\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 976us/step - loss: 0.3708 - val_loss: 0.4370\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 961us/step - loss: 0.3707 - val_loss: 0.4358\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3693 - val_loss: 0.4376\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3696 - val_loss: 0.4348\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1000us/step - loss: 0.3688 - val_loss: 0.4375\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 982us/step - loss: 0.3688 - val_loss: 0.4338\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 936us/step - loss: 0.3681 - val_loss: 0.4345\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 964us/step - loss: 0.3676 - val_loss: 0.4361\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.3670 - val_loss: 0.4339\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 980us/step - loss: 0.3661 - val_loss: 0.4324\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3663 - val_loss: 0.4331\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 976us/step - loss: 0.3663 - val_loss: 0.4342\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3656 - val_loss: 0.4341\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3648 - val_loss: 0.4349\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3646 - val_loss: 0.4358\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3641 - val_loss: 0.4374\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3638 - val_loss: 0.4348\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3635 - val_loss: 0.4358\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3632 - val_loss: 0.4359\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.3624 - val_loss: 0.4372\n",
      "121/121 [==============================] - 0s 616us/step - loss: 0.3653\n",
      "[CV] END learning_rate=0.002056176266085161, n_hidden=3, n_neurons=10; total time=  17.2s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.8448 - val_loss: 11.6681\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 949us/step - loss: 6.0177 - val_loss: 9.4659\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 7.2645 - val_loss: 38.4210\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 4.3834 - val_loss: 32.8132\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 937us/step - loss: 10.6104 - val_loss: 112.6514\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 505.8994 - val_loss: 220.0841\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 160.2805 - val_loss: 486.3292\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: 1827.8245 - val_loss: 973.6487\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 924us/step - loss: 1440.0403 - val_loss: 2209.4382\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 947us/step - loss: 12288.9141 - val_loss: 5068.2515\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 947us/step - loss: 5975.6753 - val_loss: 10272.4004\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 1167.4077 - val_loss: 20458.4160\n",
      "121/121 [==============================] - 0s 585us/step - loss: 5326.8481\n",
      "[CV] END learning_rate=0.027701145849929103, n_hidden=0, n_neurons=38; total time=   3.1s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8523 - val_loss: 0.6331\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.5291 - val_loss: 0.6992\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.5098 - val_loss: 0.7184\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 931us/step - loss: 0.5184 - val_loss: 0.6844\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.5185 - val_loss: 0.6799\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 924us/step - loss: 0.5154 - val_loss: 0.6678\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 953us/step - loss: 0.5213 - val_loss: 0.6996\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 928us/step - loss: 0.5223 - val_loss: 0.6992\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 911us/step - loss: 0.5136 - val_loss: 0.6667\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 919us/step - loss: 0.5186 - val_loss: 0.5255\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.5140 - val_loss: 0.6673\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 894us/step - loss: 0.5099 - val_loss: 0.7326\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 0.5152 - val_loss: 0.7562\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.5756 - val_loss: 0.5269\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.5130 - val_loss: 0.6347\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 977us/step - loss: 0.5128 - val_loss: 0.6847\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 919us/step - loss: 0.5121 - val_loss: 0.5376\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 906us/step - loss: 0.5172 - val_loss: 0.6267\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.5145 - val_loss: 0.6829\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 903us/step - loss: 0.5057 - val_loss: 0.5500\n",
      "121/121 [==============================] - 0s 579us/step - loss: 0.6946\n",
      "[CV] END learning_rate=0.027701145849929103, n_hidden=0, n_neurons=38; total time=   4.7s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3008 - val_loss: 8.6628\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 926us/step - loss: 0.9495 - val_loss: 0.5065\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 1.6201 - val_loss: 11.0362\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 914us/step - loss: 57.3589 - val_loss: 1.6572\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 0.7655 - val_loss: 27.8007\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 906us/step - loss: 2.1624 - val_loss: 0.9048\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 891us/step - loss: 0.8233 - val_loss: 1.0185\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 1.2430 - val_loss: 0.7337\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0868 - val_loss: 0.6711\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.6072 - val_loss: 4.1008\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 902us/step - loss: 23.2369 - val_loss: 1.0336\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 907us/step - loss: 1.7431 - val_loss: 2.1550\n",
      "121/121 [==============================] - 0s 591us/step - loss: 0.9160\n",
      "[CV] END learning_rate=0.027701145849929103, n_hidden=0, n_neurons=38; total time=   3.0s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3829 - val_loss: 4.6628\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 2.1233 - val_loss: 3.8874\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 4.4011 - val_loss: 23.8087\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 3.8236 - val_loss: 32.6192\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 907us/step - loss: 12.8748 - val_loss: 143.8396\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 928us/step - loss: 608.4749 - val_loss: 411.2242\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 911us/step - loss: 353.5321 - val_loss: 1279.8385\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 4726.5781 - val_loss: 3779.0488\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 907us/step - loss: 5741.0527 - val_loss: 11879.5000\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 61381.0117 - val_loss: 39424.4336\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 920us/step - loss: 53775.0078 - val_loss: 116497.0703\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 912us/step - loss: 18807.0566 - val_loss: 349811.3125\n",
      "121/121 [==============================] - 0s 600us/step - loss: 91160.3359\n",
      "[CV] END learning_rate=0.02389511838879087, n_hidden=0, n_neurons=53; total time=   2.9s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9157 - val_loss: 0.5991\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.5227 - val_loss: 0.6819\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 918us/step - loss: 0.5074 - val_loss: 0.7116\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 0.5149 - val_loss: 0.6775\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 919us/step - loss: 0.5148 - val_loss: 0.6733\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 903us/step - loss: 0.5127 - val_loss: 0.6654\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 903us/step - loss: 0.5169 - val_loss: 0.6823\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.5167 - val_loss: 0.6879\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.5107 - val_loss: 0.6625\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5142 - val_loss: 0.5406\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 915us/step - loss: 0.5117 - val_loss: 0.6595\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.5077 - val_loss: 0.7170\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 993us/step - loss: 0.5120 - val_loss: 0.7403\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 912us/step - loss: 0.5434 - val_loss: 0.5424\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 987us/step - loss: 0.5111 - val_loss: 0.6312\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 999us/step - loss: 0.5099 - val_loss: 0.6752\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.5094 - val_loss: 0.5544\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5142 - val_loss: 0.6216\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.5114 - val_loss: 0.6726\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 949us/step - loss: 0.5046 - val_loss: 0.5678\n",
      "121/121 [==============================] - 0s 662us/step - loss: 0.7456\n",
      "[CV] END learning_rate=0.02389511838879087, n_hidden=0, n_neurons=53; total time=   4.9s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2331 - val_loss: 4.2113\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 957us/step - loss: 0.7407 - val_loss: 0.5348\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 914us/step - loss: 1.3481 - val_loss: 8.4796\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 39.6030 - val_loss: 2.7075\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 0.8230 - val_loss: 23.5638\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 3.1847 - val_loss: 2.5870\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 936us/step - loss: 1.1950 - val_loss: 2.1674\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 915us/step - loss: 3.9984 - val_loss: 1.2334\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 927us/step - loss: 1.5134 - val_loss: 1.0978\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 924us/step - loss: 0.6212 - val_loss: 3.4209\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 969us/step - loss: 16.6147 - val_loss: 1.4745\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.8426 - val_loss: 3.0337\n",
      "121/121 [==============================] - 0s 587us/step - loss: 1.0601\n",
      "[CV] END learning_rate=0.02389511838879087, n_hidden=0, n_neurons=53; total time=   3.0s\n",
      "Epoch 1/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5457 - val_loss: 0.4486\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3769 - val_loss: 0.4423\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 975us/step - loss: 0.3841 - val_loss: 0.4103\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 983us/step - loss: 0.3532 - val_loss: 0.4232\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3455 - val_loss: 0.4178\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 980us/step - loss: 0.3386 - val_loss: 0.4344\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 987us/step - loss: 0.3269 - val_loss: 0.4023\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 926us/step - loss: 0.3216 - val_loss: 0.4128\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 964us/step - loss: 0.3150 - val_loss: 0.4177\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3120 - val_loss: 0.3845\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 932us/step - loss: 0.3098 - val_loss: 0.4277\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 902us/step - loss: 0.3057 - val_loss: 0.4068\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 914us/step - loss: 0.3013 - val_loss: 0.3991\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 918us/step - loss: 0.2954 - val_loss: 0.3899\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 904us/step - loss: 0.2919 - val_loss: 0.3849\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 892us/step - loss: 0.2875 - val_loss: 0.3775\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 913us/step - loss: 0.2858 - val_loss: 0.4206\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 924us/step - loss: 0.2838 - val_loss: 0.3882\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 976us/step - loss: 0.2846 - val_loss: 0.4013\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 907us/step - loss: 0.2816 - val_loss: 0.3925\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 921us/step - loss: 0.2768 - val_loss: 0.4075\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 918us/step - loss: 0.2737 - val_loss: 0.3812\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 912us/step - loss: 0.2735 - val_loss: 0.4201\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 912us/step - loss: 0.2723 - val_loss: 0.3879\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 908us/step - loss: 0.2726 - val_loss: 0.3914\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 916us/step - loss: 0.2695 - val_loss: 0.3854\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=3,\n",
       "                   estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x000001B9D4CD2EB0&gt;,\n",
       "                   param_distributions={&#x27;learning_rate&#x27;: [0.020150103328807057,\n",
       "                                                          0.003371647925289208,\n",
       "                                                          0.008451655734199917,\n",
       "                                                          0.002234692907699335,\n",
       "                                                          0.005459971927794963,\n",
       "                                                          0.0014397037291165947,\n",
       "                                                          0.001200713225515413,\n",
       "                                                          0.004301112746991034,\n",
       "                                                          0.00297427145514977,\n",
       "                                                          0.0043270707181953...\n",
       "                                                          0.0010525354992180116,\n",
       "                                                          0.016603995849117276,\n",
       "                                                          0.0027996634262482142,\n",
       "                                                          0.0012253895527754616,\n",
       "                                                          0.0032538619702677,\n",
       "                                                          0.0017419564485412885,\n",
       "                                                          0.004033883802629821,\n",
       "                                                          0.0005083782846016648,\n",
       "                                                          0.029587090514780482,\n",
       "                                                          0.0021292811534463253, ...],\n",
       "                                        &#x27;n_hidden&#x27;: [0, 1, 2, 3],\n",
       "                                        &#x27;n_neurons&#x27;: [1, 2, 3, 4, 5, 6, 7, 8, 9,\n",
       "                                                      10, 11, 12, 13, 14, 15,\n",
       "                                                      16, 17, 18, 19, 20, 21,\n",
       "                                                      22, 23, 24, 25, 26, 27,\n",
       "                                                      28, 29, 30, ...]},\n",
       "                   verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=3,\n",
       "                   estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x000001B9D4CD2EB0&gt;,\n",
       "                   param_distributions={&#x27;learning_rate&#x27;: [0.020150103328807057,\n",
       "                                                          0.003371647925289208,\n",
       "                                                          0.008451655734199917,\n",
       "                                                          0.002234692907699335,\n",
       "                                                          0.005459971927794963,\n",
       "                                                          0.0014397037291165947,\n",
       "                                                          0.001200713225515413,\n",
       "                                                          0.004301112746991034,\n",
       "                                                          0.00297427145514977,\n",
       "                                                          0.0043270707181953...\n",
       "                                                          0.0010525354992180116,\n",
       "                                                          0.016603995849117276,\n",
       "                                                          0.0027996634262482142,\n",
       "                                                          0.0012253895527754616,\n",
       "                                                          0.0032538619702677,\n",
       "                                                          0.0017419564485412885,\n",
       "                                                          0.004033883802629821,\n",
       "                                                          0.0005083782846016648,\n",
       "                                                          0.029587090514780482,\n",
       "                                                          0.0021292811534463253, ...],\n",
       "                                        &#x27;n_hidden&#x27;: [0, 1, 2, 3],\n",
       "                                        &#x27;n_neurons&#x27;: [1, 2, 3, 4, 5, 6, 7, 8, 9,\n",
       "                                                      10, 11, 12, 13, 14, 15,\n",
       "                                                      16, 17, 18, 19, 20, 21,\n",
       "                                                      22, 23, 24, 25, 26, 27,\n",
       "                                                      28, 29, 30, ...]},\n",
       "                   verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x000001B9D4CD2EB0&gt;</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x000001B9D4CD2EB0&gt;</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=<keras.wrappers.scikit_learn.KerasRegressor object at 0x000001B9D4CD2EB0>,\n",
       "                   param_distributions={'learning_rate': [0.020150103328807057,\n",
       "                                                          0.003371647925289208,\n",
       "                                                          0.008451655734199917,\n",
       "                                                          0.002234692907699335,\n",
       "                                                          0.005459971927794963,\n",
       "                                                          0.0014397037291165947,\n",
       "                                                          0.001200713225515413,\n",
       "                                                          0.004301112746991034,\n",
       "                                                          0.00297427145514977,\n",
       "                                                          0.0043270707181953...\n",
       "                                                          0.0010525354992180116,\n",
       "                                                          0.016603995849117276,\n",
       "                                                          0.0027996634262482142,\n",
       "                                                          0.0012253895527754616,\n",
       "                                                          0.0032538619702677,\n",
       "                                                          0.0017419564485412885,\n",
       "                                                          0.004033883802629821,\n",
       "                                                          0.0005083782846016648,\n",
       "                                                          0.029587090514780482,\n",
       "                                                          0.0021292811534463253, ...],\n",
       "                                        'n_hidden': [0, 1, 2, 3],\n",
       "                                        'n_neurons': [1, 2, 3, 4, 5, 6, 7, 8, 9,\n",
       "                                                      10, 11, 12, 13, 14, 15,\n",
       "                                                      16, 17, 18, 19, 20, 21,\n",
       "                                                      22, 23, 24, 25, 26, 27,\n",
       "                                                      28, 29, 30, ...]},\n",
       "                   verbose=2)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "param_dustribs ={\n",
    "    \"n_hidden\" : [0,1,2,3],\n",
    "    \"n_neurons\": np.arange(1,100).tolist(),\n",
    "    \"learning_rate\": reciprocal(3e-4,3e-2).rvs(1000).tolist()\n",
    "}\n",
    "# based on cros validation, keras_reg is baseline\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg,param_distributions=param_dustribs,n_iter=10,cv=3,verbose=2)\n",
    "rnd_search_cv.fit(X_train,y_train,epochs=100,\n",
    "                  validation_data=[X_valid,y_valid],\n",
    "                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neurons': 57, 'n_hidden': 3, 'learning_rate': 0.022609577469652482}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.29855525493621826"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x1b99a6cc370>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_estimator_.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 708us/step - loss: 0.3213\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.32125312089920044"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.score(X_test,y_test)#rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 696us/step - loss: 0.3213\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.32125312089920044"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = rnd_search_cv.best_estimator_.model\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_31\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_86 (Dense)            (None, 57)                513       \n",
      "                                                                 \n",
      " dense_87 (Dense)            (None, 57)                3306      \n",
      "                                                                 \n",
      " dense_88 (Dense)            (None, 57)                3306      \n",
      "                                                                 \n",
      " dense_89 (Dense)            (None, 1)                 58        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,183\n",
      "Trainable params: 7,183\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "model.save('rnd_reg.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b21892ebdafe3dff83e5313b97a9ee6f28f1dc2886ff01f2f955fb4cde4b09b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
